{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Code Generation with an RNN\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on computer source code\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network.\n",
    "\n",
    "Specify which language you want to use from the following:\n",
    "\n",
    "> c\n",
    "c_sharp\n",
    "clojure\n",
    "common_lisp\n",
    "cpp\n",
    "haskell\n",
    "java\n",
    "javascript\n",
    "lua\n",
    "ocaml\n",
    "perl\n",
    "php\n",
    "python\n",
    "ruby\n",
    "scala\n",
    "scheme\n",
    "swift\n",
    "tcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 523273 characters of java code\n"
     ]
    }
   ],
   "source": [
    "language = 'java'\n",
    "extensions = ['java']\n",
    "language_dir = '../data/programming_languages/' + language\n",
    "training_text = language + '.train.txt'\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists(training_text):\n",
    "    print('Loading code from ', language_dir)\n",
    "\n",
    "    # concatenate the code examples:\n",
    "    with open(training_text, 'w') as outfile:\n",
    "        for file in os.listdir(language_dir):\n",
    "            path = language_dir + '/' + file\n",
    "            file_has_valid_extension = list(map(file.endswith, extensions))\n",
    "\n",
    "            if file_has_valid_extension:  \n",
    "                with open(path, 'rb') as infile:   \n",
    "                    outfile.write(infile.read().decode('ISO-8859-1'))\n",
    "                    outfile.write('\\n')\n",
    "                \n",
    "with open(training_text, 'r') as infile:                \n",
    "    text = infile.read()   \n",
    "    print(\"Loaded {} characters of {} code\".format(len(text), language))\n",
    "                    \n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/*\\n * Licensed to the Apache Software Foundation (ASF) under one or more\\n * contributor license agre'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34, 86, 80, 12, 86, 12, 58, 13, 30, 61, 19, 71, 61, 74, 12,  8, 24,\n",
       "       12,  8, 63, 61, 12, 89, 59, 50, 30, 63, 61, 12,  3, 24, 43,  8, 91,\n",
       "       50, 35, 61, 12, 75, 24, 17, 19, 74, 50,  8, 13, 24, 19, 12, 85, 89,\n",
       "        3, 75,  9, 12, 17, 19, 74, 61, 35, 12, 24, 19, 61, 12, 24, 35, 12,\n",
       "        4, 24, 35, 61, 80, 12, 86, 12, 30, 24, 19,  8, 35, 13, 45, 17,  8,\n",
       "       24, 35, 12,  1, 13, 30, 61, 19, 71, 61, 12, 50, 31, 35, 61], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Making training and validation batches\n",
    "\n",
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the batch size. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of characters per batch\n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 47050)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looking at the size of this array, we see that we have rows equal to the batch size. When we want to get a batch out of here, we can grab a subset of this array that contains all the rows but has a width equal to the number of steps in the sequence. The first batch looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34, 86, 80, 12, 86, 12, 58, 13, 30, 61, 19, 71, 61, 74, 12,  8, 24,\n",
       "        12,  8, 63, 61, 12, 89, 59, 50, 30, 63, 61, 12,  3, 24, 43,  8, 91,\n",
       "        50, 35, 61, 12, 75, 24, 17, 19, 74, 50,  8, 13, 24, 19, 12, 85],\n",
       "       [31,  3, 12, 68, 12, 71, 94, 16, 32, 30, 35, 61, 50,  8, 61, 62, 35,\n",
       "        50, 59, 63, 13, 30, 71, 85,  9,  2, 80, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 31,  3, 32, 71, 61,  8, 54, 61, 19, 74, 61, 35, 13, 19, 31],\n",
       "       [74, 13,  8,  3, 17, 59, 59, 24, 35,  8, 12, 68, 12,  4, 24, 30, 48,\n",
       "        85, 15, 26, 75, 75, 24, 35,  4, 71, 15, 74, 13,  8,  3, 17, 59, 59,\n",
       "        24, 35,  8, 32, 30,  1, 50, 71, 71,  9,  2, 80, 80, 18, 18, 71],\n",
       "       [12, 90, 24, 13, 74, 12,  8, 61, 71,  8, 49, 24, 15, 75, 61, 50,  8,\n",
       "        17, 35, 61, 42, 19, 20, 50,  8, 63, 44, 24, 16, 19, 74, 61, 57, 85,\n",
       "         9, 12, 27, 80, 18, 18, 24, 48, 44, 50, 35, 31, 61,  8, 85,  9],\n",
       "       [50,  8, 17, 35, 61, 16, 28,  5, 12,  4, 71, 31, 71,  9,  2, 80, 18,\n",
       "        29, 80, 80, 18, 34, 86, 86, 80, 18, 12, 86, 12,  0, 22, 39, 39, 12,\n",
       "        45, 61, 31, 13, 19, 39, 17, 71, 61, 35, 39, 74, 24, 30, 12, 39],\n",
       "       [26, 50,  8, 63, 32, 61, 57, 59, 85, 39, 35, 50, 91, 32, 50, 59, 59,\n",
       "         1, 25, 85, 64,  9,  9,  9,  2, 80, 12, 12, 12, 12, 12, 12, 89, 71,\n",
       "        71, 61, 35,  8, 32, 50, 71, 71, 61, 35,  8, 15, 46, 17, 50,  1],\n",
       "       [32, 61, 30,  1, 13, 59, 71, 61, 32, 61,  4, 43, 32, 61, 30, 59, 32,\n",
       "        90, 13, 61, 91, 32,  8, 35, 61, 61,  4, 50, 71,  8, 61, 35, 74, 61,\n",
       "         8, 50, 13,  1, 32, 90, 50,  1, 13, 74, 50,  8, 13, 24, 19, 32],\n",
       "       [ 8, 61, 35,  0, 34, 45, 66,  0, 34, 61,  4, 66, 70, 32, 80, 12, 86,\n",
       "        12,  0, 22, 39, 39, 12, 61, 19, 74, 39, 17, 71, 61, 35, 39, 74, 24,\n",
       "        30, 12, 39, 39, 66, 80, 12, 86, 80, 12, 86, 12,  0, 59, 66, 80],\n",
       "       [12, 71, 61,  8, 80, 18, 12, 86, 34, 80, 18, 59, 17, 45,  1, 13, 30,\n",
       "        12, 90, 24, 13, 74, 12, 71, 61,  8, 75, 13, 35, 71,  8, 75, 13, 35,\n",
       "        71,  8, 83, 63, 13,  1, 74, 85, 60, 15,  1, 61,  4, 61, 19,  8],\n",
       "       [12, 12,  8, 63, 13, 71, 32, 50, 12, 68, 12, 50,  2, 80, 12, 12, 12,\n",
       "        12, 29, 80, 80, 12, 12, 12, 12, 53, 42, 90, 61, 35, 35, 13, 74, 61,\n",
       "        80, 12, 12, 12, 12, 59, 17, 45,  1, 13, 30, 12, 45, 24, 24,  1]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I'll write another function to grab batches out of the arrays made by `split_data`. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building the model\n",
    "\n",
    "Below is a function where I build the graph for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "    ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    ### Run the data through the RNN layers\n",
    "    # This makes a list where each element is on step in the sequence\n",
    "    rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "    \n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    # NOTE: I'm using a namedtuple here because I think they are cool\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. \n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to write it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that heps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}_v{validation loss}.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100  Iteration 1/4600 Training loss: 4.5614 4.2672 sec/batch\n",
      "Epoch 1/100  Iteration 2/4600 Training loss: 4.5250 3.0903 sec/batch\n",
      "Epoch 1/100  Iteration 3/4600 Training loss: 4.3925 3.0713 sec/batch\n",
      "Epoch 1/100  Iteration 4/4600 Training loss: 5.0468 3.0902 sec/batch\n",
      "Epoch 1/100  Iteration 5/4600 Training loss: 5.0088 3.1290 sec/batch\n",
      "Epoch 1/100  Iteration 6/4600 Training loss: 4.8697 3.1066 sec/batch\n",
      "Epoch 1/100  Iteration 7/4600 Training loss: 4.7539 3.1085 sec/batch\n",
      "Epoch 1/100  Iteration 8/4600 Training loss: 4.6536 3.0892 sec/batch\n",
      "Epoch 1/100  Iteration 9/4600 Training loss: 4.5668 3.1233 sec/batch\n",
      "Epoch 1/100  Iteration 10/4600 Training loss: 4.4952 3.1021 sec/batch\n",
      "Epoch 1/100  Iteration 11/4600 Training loss: 4.4286 3.1103 sec/batch\n",
      "Epoch 1/100  Iteration 12/4600 Training loss: 4.3724 3.1069 sec/batch\n",
      "Epoch 1/100  Iteration 13/4600 Training loss: 4.3208 3.1041 sec/batch\n",
      "Epoch 1/100  Iteration 14/4600 Training loss: 4.2782 3.1104 sec/batch\n",
      "Epoch 1/100  Iteration 15/4600 Training loss: 4.2406 3.0886 sec/batch\n",
      "Epoch 1/100  Iteration 16/4600 Training loss: 4.2057 3.0981 sec/batch\n",
      "Epoch 1/100  Iteration 17/4600 Training loss: 4.1751 3.0965 sec/batch\n",
      "Epoch 1/100  Iteration 18/4600 Training loss: 4.1470 3.1159 sec/batch\n",
      "Epoch 1/100  Iteration 19/4600 Training loss: 4.1229 3.0863 sec/batch\n",
      "Epoch 1/100  Iteration 20/4600 Training loss: 4.0990 3.0674 sec/batch\n",
      "Epoch 1/100  Iteration 21/4600 Training loss: 4.0766 3.1174 sec/batch\n",
      "Epoch 1/100  Iteration 22/4600 Training loss: 4.0562 3.0991 sec/batch\n",
      "Epoch 1/100  Iteration 23/4600 Training loss: 4.0359 3.0709 sec/batch\n",
      "Epoch 1/100  Iteration 24/4600 Training loss: 4.0168 3.0949 sec/batch\n",
      "Epoch 1/100  Iteration 25/4600 Training loss: 4.0019 3.1204 sec/batch\n",
      "Epoch 1/100  Iteration 26/4600 Training loss: 3.9873 3.1327 sec/batch\n",
      "Epoch 1/100  Iteration 27/4600 Training loss: 3.9734 3.0984 sec/batch\n",
      "Epoch 1/100  Iteration 28/4600 Training loss: 3.9575 3.0844 sec/batch\n",
      "Epoch 1/100  Iteration 29/4600 Training loss: 3.9445 3.1146 sec/batch\n",
      "Epoch 1/100  Iteration 30/4600 Training loss: 3.9327 3.0812 sec/batch\n",
      "Epoch 1/100  Iteration 31/4600 Training loss: 3.9211 3.0867 sec/batch\n",
      "Epoch 1/100  Iteration 32/4600 Training loss: 3.9090 3.1066 sec/batch\n",
      "Epoch 1/100  Iteration 33/4600 Training loss: 3.8949 3.1070 sec/batch\n",
      "Epoch 1/100  Iteration 34/4600 Training loss: 3.8831 3.1015 sec/batch\n",
      "Epoch 1/100  Iteration 35/4600 Training loss: 3.8702 3.0939 sec/batch\n",
      "Epoch 1/100  Iteration 36/4600 Training loss: 3.8600 3.1105 sec/batch\n",
      "Epoch 1/100  Iteration 37/4600 Training loss: 3.8488 3.1315 sec/batch\n",
      "Epoch 1/100  Iteration 38/4600 Training loss: 3.8398 3.1128 sec/batch\n",
      "Epoch 1/100  Iteration 39/4600 Training loss: 3.8307 3.0946 sec/batch\n",
      "Epoch 1/100  Iteration 40/4600 Training loss: 3.8222 3.1192 sec/batch\n",
      "Epoch 1/100  Iteration 41/4600 Training loss: 3.8156 3.0843 sec/batch\n",
      "Epoch 1/100  Iteration 42/4600 Training loss: 3.8077 3.0937 sec/batch\n",
      "Epoch 1/100  Iteration 43/4600 Training loss: 3.7999 3.1226 sec/batch\n",
      "Epoch 1/100  Iteration 44/4600 Training loss: 3.7925 3.0911 sec/batch\n",
      "Epoch 1/100  Iteration 45/4600 Training loss: 3.7866 3.1023 sec/batch\n",
      "Epoch 1/100  Iteration 46/4600 Training loss: 3.7797 3.1205 sec/batch\n",
      "Epoch 2/100  Iteration 47/4600 Training loss: 3.4960 3.1569 sec/batch\n",
      "Epoch 2/100  Iteration 48/4600 Training loss: 3.5063 3.1680 sec/batch\n",
      "Epoch 2/100  Iteration 49/4600 Training loss: 3.4942 3.0991 sec/batch\n",
      "Epoch 2/100  Iteration 50/4600 Training loss: 3.4818 3.0787 sec/batch\n",
      "Epoch 2/100  Iteration 51/4600 Training loss: 3.4696 3.1147 sec/batch\n",
      "Epoch 2/100  Iteration 52/4600 Training loss: 3.4580 3.1171 sec/batch\n",
      "Epoch 2/100  Iteration 53/4600 Training loss: 3.4630 3.1050 sec/batch\n",
      "Epoch 2/100  Iteration 54/4600 Training loss: 3.4584 3.1253 sec/batch\n",
      "Epoch 2/100  Iteration 55/4600 Training loss: 3.4545 3.0890 sec/batch\n",
      "Epoch 2/100  Iteration 56/4600 Training loss: 3.4555 3.1018 sec/batch\n",
      "Epoch 2/100  Iteration 57/4600 Training loss: 3.4498 3.0950 sec/batch\n",
      "Epoch 2/100  Iteration 58/4600 Training loss: 3.4479 3.1456 sec/batch\n",
      "Epoch 2/100  Iteration 59/4600 Training loss: 3.4427 3.1646 sec/batch\n",
      "Epoch 2/100  Iteration 60/4600 Training loss: 3.4416 3.1402 sec/batch\n",
      "Epoch 2/100  Iteration 61/4600 Training loss: 3.4415 3.0882 sec/batch\n",
      "Epoch 2/100  Iteration 62/4600 Training loss: 3.4410 3.0978 sec/batch\n",
      "Epoch 2/100  Iteration 63/4600 Training loss: 3.4405 3.0687 sec/batch\n",
      "Epoch 2/100  Iteration 64/4600 Training loss: 3.4384 3.0876 sec/batch\n",
      "Epoch 2/100  Iteration 65/4600 Training loss: 3.4369 3.1017 sec/batch\n",
      "Epoch 2/100  Iteration 66/4600 Training loss: 3.4356 3.0900 sec/batch\n",
      "Epoch 2/100  Iteration 67/4600 Training loss: 3.4348 3.1089 sec/batch\n",
      "Epoch 2/100  Iteration 68/4600 Training loss: 3.4331 3.0815 sec/batch\n",
      "Epoch 2/100  Iteration 69/4600 Training loss: 3.4303 3.1040 sec/batch\n",
      "Epoch 2/100  Iteration 70/4600 Training loss: 3.4252 3.1085 sec/batch\n",
      "Epoch 2/100  Iteration 71/4600 Training loss: 3.4238 3.1047 sec/batch\n",
      "Epoch 2/100  Iteration 72/4600 Training loss: 3.4214 3.1081 sec/batch\n",
      "Epoch 2/100  Iteration 73/4600 Training loss: 3.4188 3.1058 sec/batch\n",
      "Epoch 2/100  Iteration 74/4600 Training loss: 3.4139 3.0985 sec/batch\n",
      "Epoch 2/100  Iteration 75/4600 Training loss: 3.4111 3.1270 sec/batch\n",
      "Epoch 2/100  Iteration 76/4600 Training loss: 3.4091 3.1366 sec/batch\n",
      "Epoch 2/100  Iteration 77/4600 Training loss: 3.4074 3.0893 sec/batch\n",
      "Epoch 2/100  Iteration 78/4600 Training loss: 3.4040 3.1127 sec/batch\n",
      "Epoch 2/100  Iteration 79/4600 Training loss: 3.3989 3.1116 sec/batch\n",
      "Epoch 2/100  Iteration 80/4600 Training loss: 3.3953 3.1219 sec/batch\n",
      "Epoch 2/100  Iteration 81/4600 Training loss: 3.3904 3.1112 sec/batch\n",
      "Epoch 2/100  Iteration 82/4600 Training loss: 3.3880 3.1106 sec/batch\n",
      "Epoch 2/100  Iteration 83/4600 Training loss: 3.3840 3.1038 sec/batch\n",
      "Epoch 2/100  Iteration 84/4600 Training loss: 3.3816 3.1105 sec/batch\n",
      "Epoch 2/100  Iteration 85/4600 Training loss: 3.3786 3.1243 sec/batch\n",
      "Epoch 2/100  Iteration 86/4600 Training loss: 3.3761 3.1124 sec/batch\n",
      "Epoch 2/100  Iteration 87/4600 Training loss: 3.3753 3.1144 sec/batch\n",
      "Epoch 2/100  Iteration 88/4600 Training loss: 3.3730 3.0984 sec/batch\n",
      "Epoch 2/100  Iteration 89/4600 Training loss: 3.3708 3.0991 sec/batch\n",
      "Epoch 2/100  Iteration 90/4600 Training loss: 3.3688 3.0889 sec/batch\n",
      "Epoch 2/100  Iteration 91/4600 Training loss: 3.3680 3.1161 sec/batch\n",
      "Epoch 2/100  Iteration 92/4600 Training loss: 3.3657 3.1471 sec/batch\n",
      "Epoch 3/100  Iteration 93/4600 Training loss: 3.2927 3.1321 sec/batch\n",
      "Epoch 3/100  Iteration 94/4600 Training loss: 3.2969 3.1565 sec/batch\n",
      "Epoch 3/100  Iteration 95/4600 Training loss: 3.2852 3.1140 sec/batch\n",
      "Epoch 3/100  Iteration 96/4600 Training loss: 3.2736 3.1192 sec/batch\n",
      "Epoch 3/100  Iteration 97/4600 Training loss: 3.2583 3.1028 sec/batch\n",
      "Epoch 3/100  Iteration 98/4600 Training loss: 3.2481 3.0924 sec/batch\n",
      "Epoch 3/100  Iteration 99/4600 Training loss: 3.2526 3.0902 sec/batch\n",
      "Epoch 3/100  Iteration 100/4600 Training loss: 3.2501 3.0901 sec/batch\n",
      "Epoch 3/100  Iteration 101/4600 Training loss: 3.2453 3.0967 sec/batch\n",
      "Epoch 3/100  Iteration 102/4600 Training loss: 3.2462 3.0848 sec/batch\n",
      "Epoch 3/100  Iteration 103/4600 Training loss: 3.2393 3.0885 sec/batch\n",
      "Epoch 3/100  Iteration 104/4600 Training loss: 3.2381 3.0832 sec/batch\n",
      "Epoch 3/100  Iteration 105/4600 Training loss: 3.2328 3.0636 sec/batch\n",
      "Epoch 3/100  Iteration 106/4600 Training loss: 3.2328 3.0764 sec/batch\n",
      "Epoch 3/100  Iteration 107/4600 Training loss: 3.2330 3.0819 sec/batch\n",
      "Epoch 3/100  Iteration 108/4600 Training loss: 3.2324 3.0744 sec/batch\n",
      "Epoch 3/100  Iteration 109/4600 Training loss: 3.2315 3.0896 sec/batch\n",
      "Epoch 3/100  Iteration 110/4600 Training loss: 3.2307 3.0744 sec/batch\n",
      "Epoch 3/100  Iteration 111/4600 Training loss: 3.2301 3.0851 sec/batch\n",
      "Epoch 3/100  Iteration 112/4600 Training loss: 3.2294 3.1135 sec/batch\n",
      "Epoch 3/100  Iteration 113/4600 Training loss: 3.2294 3.0677 sec/batch\n",
      "Epoch 3/100  Iteration 114/4600 Training loss: 3.2278 3.0958 sec/batch\n",
      "Epoch 3/100  Iteration 115/4600 Training loss: 3.2254 3.1047 sec/batch\n",
      "Epoch 3/100  Iteration 116/4600 Training loss: 3.2212 3.1012 sec/batch\n",
      "Epoch 3/100  Iteration 117/4600 Training loss: 3.2206 3.1154 sec/batch\n",
      "Epoch 3/100  Iteration 118/4600 Training loss: 3.2190 3.0995 sec/batch\n",
      "Epoch 3/100  Iteration 119/4600 Training loss: 3.2170 3.0986 sec/batch\n",
      "Epoch 3/100  Iteration 120/4600 Training loss: 3.2130 3.0994 sec/batch\n",
      "Epoch 3/100  Iteration 121/4600 Training loss: 3.2121 3.0823 sec/batch\n",
      "Epoch 3/100  Iteration 122/4600 Training loss: 3.2112 3.0857 sec/batch\n",
      "Epoch 3/100  Iteration 123/4600 Training loss: 3.2107 3.0728 sec/batch\n",
      "Epoch 3/100  Iteration 124/4600 Training loss: 3.2088 3.0863 sec/batch\n",
      "Epoch 3/100  Iteration 125/4600 Training loss: 3.2046 3.0638 sec/batch\n",
      "Epoch 3/100  Iteration 126/4600 Training loss: 3.2018 3.0634 sec/batch\n",
      "Epoch 3/100  Iteration 127/4600 Training loss: 3.1980 3.0937 sec/batch\n",
      "Epoch 3/100  Iteration 128/4600 Training loss: 3.1966 3.0860 sec/batch\n",
      "Epoch 3/100  Iteration 129/4600 Training loss: 3.1937 3.1060 sec/batch\n",
      "Epoch 3/100  Iteration 130/4600 Training loss: 3.1920 3.0942 sec/batch\n",
      "Epoch 3/100  Iteration 131/4600 Training loss: 3.1898 3.1139 sec/batch\n",
      "Epoch 3/100  Iteration 132/4600 Training loss: 3.1880 3.1093 sec/batch\n",
      "Epoch 3/100  Iteration 133/4600 Training loss: 3.1875 3.1059 sec/batch\n",
      "Epoch 3/100  Iteration 134/4600 Training loss: 3.1856 3.1011 sec/batch\n",
      "Epoch 3/100  Iteration 135/4600 Training loss: 3.1841 3.0718 sec/batch\n",
      "Epoch 3/100  Iteration 136/4600 Training loss: 3.1823 3.1067 sec/batch\n",
      "Epoch 3/100  Iteration 137/4600 Training loss: 3.1820 3.0504 sec/batch\n",
      "Epoch 3/100  Iteration 138/4600 Training loss: 3.1804 3.1120 sec/batch\n",
      "Epoch 4/100  Iteration 139/4600 Training loss: 3.1372 3.0933 sec/batch\n",
      "Epoch 4/100  Iteration 140/4600 Training loss: 3.1371 3.0687 sec/batch\n",
      "Epoch 4/100  Iteration 141/4600 Training loss: 3.1281 3.1011 sec/batch\n",
      "Epoch 4/100  Iteration 142/4600 Training loss: 3.1176 3.0737 sec/batch\n",
      "Epoch 4/100  Iteration 143/4600 Training loss: 3.0998 3.0910 sec/batch\n",
      "Epoch 4/100  Iteration 144/4600 Training loss: 3.0913 3.0915 sec/batch\n",
      "Epoch 4/100  Iteration 145/4600 Training loss: 3.0962 3.0843 sec/batch\n",
      "Epoch 4/100  Iteration 146/4600 Training loss: 3.0938 3.0803 sec/batch\n",
      "Epoch 4/100  Iteration 147/4600 Training loss: 3.0890 3.0751 sec/batch\n",
      "Epoch 4/100  Iteration 148/4600 Training loss: 3.0885 3.0731 sec/batch\n",
      "Epoch 4/100  Iteration 149/4600 Training loss: 3.0827 3.1293 sec/batch\n",
      "Epoch 4/100  Iteration 150/4600 Training loss: 3.0814 3.1636 sec/batch\n",
      "Epoch 4/100  Iteration 151/4600 Training loss: 3.0759 3.0945 sec/batch\n",
      "Epoch 4/100  Iteration 152/4600 Training loss: 3.0762 3.0733 sec/batch\n",
      "Epoch 4/100  Iteration 153/4600 Training loss: 3.0766 3.0882 sec/batch\n",
      "Epoch 4/100  Iteration 154/4600 Training loss: 3.0757 3.0723 sec/batch\n",
      "Epoch 4/100  Iteration 155/4600 Training loss: 3.0746 3.0613 sec/batch\n",
      "Epoch 4/100  Iteration 156/4600 Training loss: 3.0737 3.0730 sec/batch\n",
      "Epoch 4/100  Iteration 157/4600 Training loss: 3.0733 3.1246 sec/batch\n",
      "Epoch 4/100  Iteration 158/4600 Training loss: 3.0719 3.1375 sec/batch\n",
      "Epoch 4/100  Iteration 159/4600 Training loss: 3.0718 3.1039 sec/batch\n",
      "Epoch 4/100  Iteration 160/4600 Training loss: 3.0698 3.1198 sec/batch\n",
      "Epoch 4/100  Iteration 161/4600 Training loss: 3.0678 3.0812 sec/batch\n",
      "Epoch 4/100  Iteration 162/4600 Training loss: 3.0633 3.0904 sec/batch\n",
      "Epoch 4/100  Iteration 163/4600 Training loss: 3.0621 3.0689 sec/batch\n",
      "Epoch 4/100  Iteration 164/4600 Training loss: 3.0607 3.1000 sec/batch\n",
      "Epoch 4/100  Iteration 165/4600 Training loss: 3.0586 3.0973 sec/batch\n",
      "Epoch 4/100  Iteration 166/4600 Training loss: 3.0546 3.1163 sec/batch\n",
      "Epoch 4/100  Iteration 167/4600 Training loss: 3.0540 3.0904 sec/batch\n",
      "Epoch 4/100  Iteration 168/4600 Training loss: 3.0530 3.0841 sec/batch\n",
      "Epoch 4/100  Iteration 169/4600 Training loss: 3.0521 3.0914 sec/batch\n",
      "Epoch 4/100  Iteration 170/4600 Training loss: 3.0497 3.0912 sec/batch\n",
      "Epoch 4/100  Iteration 171/4600 Training loss: 3.0453 3.1054 sec/batch\n",
      "Epoch 4/100  Iteration 172/4600 Training loss: 3.0419 3.1060 sec/batch\n",
      "Epoch 4/100  Iteration 173/4600 Training loss: 3.0383 3.0778 sec/batch\n",
      "Epoch 4/100  Iteration 174/4600 Training loss: 3.0367 3.0766 sec/batch\n",
      "Epoch 4/100  Iteration 175/4600 Training loss: 3.0334 3.0920 sec/batch\n",
      "Epoch 4/100  Iteration 176/4600 Training loss: 3.0314 3.0809 sec/batch\n",
      "Epoch 4/100  Iteration 177/4600 Training loss: 3.0284 3.0767 sec/batch\n",
      "Epoch 4/100  Iteration 178/4600 Training loss: 3.0260 3.0977 sec/batch\n",
      "Epoch 4/100  Iteration 179/4600 Training loss: 3.0250 3.1396 sec/batch\n",
      "Epoch 4/100  Iteration 180/4600 Training loss: 3.0224 3.1243 sec/batch\n",
      "Epoch 4/100  Iteration 181/4600 Training loss: 3.0204 3.0657 sec/batch\n",
      "Epoch 4/100  Iteration 182/4600 Training loss: 3.0185 3.0834 sec/batch\n",
      "Epoch 4/100  Iteration 183/4600 Training loss: 3.0176 3.0974 sec/batch\n",
      "Epoch 4/100  Iteration 184/4600 Training loss: 3.0152 3.1108 sec/batch\n",
      "Epoch 5/100  Iteration 185/4600 Training loss: 2.9683 3.1137 sec/batch\n",
      "Epoch 5/100  Iteration 186/4600 Training loss: 2.9710 3.0935 sec/batch\n",
      "Epoch 5/100  Iteration 187/4600 Training loss: 2.9541 3.0938 sec/batch\n",
      "Epoch 5/100  Iteration 188/4600 Training loss: 2.9384 3.0782 sec/batch\n",
      "Epoch 5/100  Iteration 189/4600 Training loss: 2.9216 3.0754 sec/batch\n",
      "Epoch 5/100  Iteration 190/4600 Training loss: 2.9125 3.0686 sec/batch\n",
      "Epoch 5/100  Iteration 191/4600 Training loss: 2.9157 3.1085 sec/batch\n",
      "Epoch 5/100  Iteration 192/4600 Training loss: 2.9130 3.0712 sec/batch\n",
      "Epoch 5/100  Iteration 193/4600 Training loss: 2.9059 3.0880 sec/batch\n",
      "Epoch 5/100  Iteration 194/4600 Training loss: 2.9038 3.0793 sec/batch\n",
      "Epoch 5/100  Iteration 195/4600 Training loss: 2.8958 3.0917 sec/batch\n",
      "Epoch 5/100  Iteration 196/4600 Training loss: 2.8914 3.0885 sec/batch\n",
      "Epoch 5/100  Iteration 197/4600 Training loss: 2.8849 3.0928 sec/batch\n",
      "Epoch 5/100  Iteration 198/4600 Training loss: 2.8846 3.0797 sec/batch\n",
      "Epoch 5/100  Iteration 199/4600 Training loss: 2.8843 3.1044 sec/batch\n",
      "Epoch 5/100  Iteration 200/4600 Training loss: 2.8823 3.0985 sec/batch\n",
      "Validation loss: 2.78648 Saving checkpoint!\n",
      "Epoch 5/100  Iteration 201/4600 Training loss: 2.8836 3.0929 sec/batch\n",
      "Epoch 5/100  Iteration 202/4600 Training loss: 2.8820 3.1199 sec/batch\n",
      "Epoch 5/100  Iteration 203/4600 Training loss: 2.8807 3.1123 sec/batch\n",
      "Epoch 5/100  Iteration 204/4600 Training loss: 2.8787 3.1050 sec/batch\n",
      "Epoch 5/100  Iteration 205/4600 Training loss: 2.8778 3.0620 sec/batch\n",
      "Epoch 5/100  Iteration 206/4600 Training loss: 2.8755 3.0913 sec/batch\n",
      "Epoch 5/100  Iteration 207/4600 Training loss: 2.8728 3.0926 sec/batch\n",
      "Epoch 5/100  Iteration 208/4600 Training loss: 2.8679 3.1055 sec/batch\n",
      "Epoch 5/100  Iteration 209/4600 Training loss: 2.8663 3.0759 sec/batch\n",
      "Epoch 5/100  Iteration 210/4600 Training loss: 2.8629 3.0897 sec/batch\n",
      "Epoch 5/100  Iteration 211/4600 Training loss: 2.8596 3.0872 sec/batch\n",
      "Epoch 5/100  Iteration 212/4600 Training loss: 2.8551 3.0784 sec/batch\n",
      "Epoch 5/100  Iteration 213/4600 Training loss: 2.8535 3.0708 sec/batch\n",
      "Epoch 5/100  Iteration 214/4600 Training loss: 2.8514 3.0871 sec/batch\n",
      "Epoch 5/100  Iteration 215/4600 Training loss: 2.8491 3.1273 sec/batch\n",
      "Epoch 5/100  Iteration 216/4600 Training loss: 2.8455 3.1315 sec/batch\n",
      "Epoch 5/100  Iteration 217/4600 Training loss: 2.8399 3.1070 sec/batch\n",
      "Epoch 5/100  Iteration 218/4600 Training loss: 2.8358 3.1054 sec/batch\n",
      "Epoch 5/100  Iteration 219/4600 Training loss: 2.8314 3.0551 sec/batch\n",
      "Epoch 5/100  Iteration 220/4600 Training loss: 2.8289 3.0772 sec/batch\n",
      "Epoch 5/100  Iteration 221/4600 Training loss: 2.8246 3.0891 sec/batch\n",
      "Epoch 5/100  Iteration 222/4600 Training loss: 2.8215 3.0904 sec/batch\n",
      "Epoch 5/100  Iteration 223/4600 Training loss: 2.8176 3.1016 sec/batch\n",
      "Epoch 5/100  Iteration 224/4600 Training loss: 2.8149 3.0740 sec/batch\n",
      "Epoch 5/100  Iteration 225/4600 Training loss: 2.8134 3.0736 sec/batch\n",
      "Epoch 5/100  Iteration 226/4600 Training loss: 2.8110 3.0881 sec/batch\n",
      "Epoch 5/100  Iteration 227/4600 Training loss: 2.8086 3.0937 sec/batch\n",
      "Epoch 5/100  Iteration 228/4600 Training loss: 2.8063 3.0953 sec/batch\n",
      "Epoch 5/100  Iteration 229/4600 Training loss: 2.8051 3.0918 sec/batch\n",
      "Epoch 5/100  Iteration 230/4600 Training loss: 2.8023 3.1038 sec/batch\n",
      "Epoch 6/100  Iteration 231/4600 Training loss: 2.7300 3.0927 sec/batch\n",
      "Epoch 6/100  Iteration 232/4600 Training loss: 2.7337 3.1078 sec/batch\n",
      "Epoch 6/100  Iteration 233/4600 Training loss: 2.7246 3.1062 sec/batch\n",
      "Epoch 6/100  Iteration 234/4600 Training loss: 2.7079 3.0688 sec/batch\n",
      "Epoch 6/100  Iteration 235/4600 Training loss: 2.6883 3.0961 sec/batch\n",
      "Epoch 6/100  Iteration 236/4600 Training loss: 2.6788 3.0870 sec/batch\n",
      "Epoch 6/100  Iteration 237/4600 Training loss: 2.6786 3.0808 sec/batch\n",
      "Epoch 6/100  Iteration 238/4600 Training loss: 2.6733 3.0973 sec/batch\n",
      "Epoch 6/100  Iteration 239/4600 Training loss: 2.6634 3.1248 sec/batch\n",
      "Epoch 6/100  Iteration 240/4600 Training loss: 2.6583 3.1074 sec/batch\n",
      "Epoch 6/100  Iteration 241/4600 Training loss: 2.6484 3.1110 sec/batch\n",
      "Epoch 6/100  Iteration 242/4600 Training loss: 2.6412 3.0692 sec/batch\n",
      "Epoch 6/100  Iteration 243/4600 Training loss: 2.6322 3.1137 sec/batch\n",
      "Epoch 6/100  Iteration 244/4600 Training loss: 2.6305 3.0992 sec/batch\n",
      "Epoch 6/100  Iteration 245/4600 Training loss: 2.6287 3.0861 sec/batch\n",
      "Epoch 6/100  Iteration 246/4600 Training loss: 2.6262 3.0877 sec/batch\n",
      "Epoch 6/100  Iteration 247/4600 Training loss: 2.6231 3.0717 sec/batch\n",
      "Epoch 6/100  Iteration 248/4600 Training loss: 2.6215 3.0609 sec/batch\n",
      "Epoch 6/100  Iteration 249/4600 Training loss: 2.6198 3.0888 sec/batch\n",
      "Epoch 6/100  Iteration 250/4600 Training loss: 2.6160 3.0746 sec/batch\n",
      "Epoch 6/100  Iteration 251/4600 Training loss: 2.6155 3.0957 sec/batch\n",
      "Epoch 6/100  Iteration 252/4600 Training loss: 2.6141 3.1218 sec/batch\n",
      "Epoch 6/100  Iteration 253/4600 Training loss: 2.6130 3.0880 sec/batch\n",
      "Epoch 6/100  Iteration 254/4600 Training loss: 2.6094 3.0811 sec/batch\n",
      "Epoch 6/100  Iteration 255/4600 Training loss: 2.6094 3.1255 sec/batch\n",
      "Epoch 6/100  Iteration 256/4600 Training loss: 2.6069 3.1287 sec/batch\n",
      "Epoch 6/100  Iteration 257/4600 Training loss: 2.6047 3.1462 sec/batch\n",
      "Epoch 6/100  Iteration 258/4600 Training loss: 2.6019 3.1251 sec/batch\n",
      "Epoch 6/100  Iteration 259/4600 Training loss: 2.6007 3.0912 sec/batch\n",
      "Epoch 6/100  Iteration 260/4600 Training loss: 2.5983 3.0776 sec/batch\n",
      "Epoch 6/100  Iteration 261/4600 Training loss: 2.5969 3.1044 sec/batch\n",
      "Epoch 6/100  Iteration 262/4600 Training loss: 2.5937 3.0937 sec/batch\n",
      "Epoch 6/100  Iteration 263/4600 Training loss: 2.5890 3.0943 sec/batch\n",
      "Epoch 6/100  Iteration 264/4600 Training loss: 2.5843 3.1104 sec/batch\n",
      "Epoch 6/100  Iteration 265/4600 Training loss: 2.5800 3.1042 sec/batch\n",
      "Epoch 6/100  Iteration 266/4600 Training loss: 2.5774 3.1254 sec/batch\n",
      "Epoch 6/100  Iteration 267/4600 Training loss: 2.5735 3.0707 sec/batch\n",
      "Epoch 6/100  Iteration 268/4600 Training loss: 2.5708 3.0638 sec/batch\n",
      "Epoch 6/100  Iteration 269/4600 Training loss: 2.5672 3.0678 sec/batch\n",
      "Epoch 6/100  Iteration 270/4600 Training loss: 2.5637 3.0976 sec/batch\n",
      "Epoch 6/100  Iteration 271/4600 Training loss: 2.5619 3.0791 sec/batch\n",
      "Epoch 6/100  Iteration 272/4600 Training loss: 2.5590 3.0637 sec/batch\n",
      "Epoch 6/100  Iteration 273/4600 Training loss: 2.5568 3.0862 sec/batch\n",
      "Epoch 6/100  Iteration 274/4600 Training loss: 2.5544 3.0742 sec/batch\n",
      "Epoch 6/100  Iteration 275/4600 Training loss: 2.5530 3.0821 sec/batch\n",
      "Epoch 6/100  Iteration 276/4600 Training loss: 2.5503 3.0677 sec/batch\n",
      "Epoch 7/100  Iteration 277/4600 Training loss: 2.5108 3.0837 sec/batch\n",
      "Epoch 7/100  Iteration 278/4600 Training loss: 2.5067 3.1026 sec/batch\n",
      "Epoch 7/100  Iteration 279/4600 Training loss: 2.4929 3.0804 sec/batch\n",
      "Epoch 7/100  Iteration 280/4600 Training loss: 2.4775 3.0554 sec/batch\n",
      "Epoch 7/100  Iteration 281/4600 Training loss: 2.4577 3.1162 sec/batch\n",
      "Epoch 7/100  Iteration 282/4600 Training loss: 2.4480 3.0528 sec/batch\n",
      "Epoch 7/100  Iteration 283/4600 Training loss: 2.4470 3.0880 sec/batch\n",
      "Epoch 7/100  Iteration 284/4600 Training loss: 2.4430 3.0943 sec/batch\n",
      "Epoch 7/100  Iteration 285/4600 Training loss: 2.4326 3.1015 sec/batch\n",
      "Epoch 7/100  Iteration 286/4600 Training loss: 2.4276 3.0675 sec/batch\n",
      "Epoch 7/100  Iteration 287/4600 Training loss: 2.4190 3.0782 sec/batch\n",
      "Epoch 7/100  Iteration 288/4600 Training loss: 2.4113 3.0828 sec/batch\n",
      "Epoch 7/100  Iteration 289/4600 Training loss: 2.4028 3.0804 sec/batch\n",
      "Epoch 7/100  Iteration 290/4600 Training loss: 2.4017 3.0725 sec/batch\n",
      "Epoch 7/100  Iteration 291/4600 Training loss: 2.3986 3.0820 sec/batch\n",
      "Epoch 7/100  Iteration 292/4600 Training loss: 2.3955 3.0814 sec/batch\n",
      "Epoch 7/100  Iteration 293/4600 Training loss: 2.3928 3.1037 sec/batch\n",
      "Epoch 7/100  Iteration 294/4600 Training loss: 2.3913 3.0894 sec/batch\n",
      "Epoch 7/100  Iteration 295/4600 Training loss: 2.3895 3.0929 sec/batch\n",
      "Epoch 7/100  Iteration 296/4600 Training loss: 2.3860 3.0721 sec/batch\n",
      "Epoch 7/100  Iteration 297/4600 Training loss: 2.3848 3.0826 sec/batch\n",
      "Epoch 7/100  Iteration 298/4600 Training loss: 2.3831 3.0806 sec/batch\n",
      "Epoch 7/100  Iteration 299/4600 Training loss: 2.3826 3.0885 sec/batch\n",
      "Epoch 7/100  Iteration 300/4600 Training loss: 2.3791 3.0970 sec/batch\n",
      "Epoch 7/100  Iteration 301/4600 Training loss: 2.3784 3.0953 sec/batch\n",
      "Epoch 7/100  Iteration 302/4600 Training loss: 2.3761 3.0550 sec/batch\n",
      "Epoch 7/100  Iteration 303/4600 Training loss: 2.3747 3.0867 sec/batch\n",
      "Epoch 7/100  Iteration 304/4600 Training loss: 2.3727 3.0673 sec/batch\n",
      "Epoch 7/100  Iteration 305/4600 Training loss: 2.3725 3.0757 sec/batch\n",
      "Epoch 7/100  Iteration 306/4600 Training loss: 2.3706 3.0853 sec/batch\n",
      "Epoch 7/100  Iteration 307/4600 Training loss: 2.3698 3.0815 sec/batch\n",
      "Epoch 7/100  Iteration 308/4600 Training loss: 2.3679 3.0815 sec/batch\n",
      "Epoch 7/100  Iteration 309/4600 Training loss: 2.3646 3.0738 sec/batch\n",
      "Epoch 7/100  Iteration 310/4600 Training loss: 2.3612 3.1001 sec/batch\n",
      "Epoch 7/100  Iteration 311/4600 Training loss: 2.3575 3.0639 sec/batch\n",
      "Epoch 7/100  Iteration 312/4600 Training loss: 2.3555 3.0892 sec/batch\n",
      "Epoch 7/100  Iteration 313/4600 Training loss: 2.3523 3.0670 sec/batch\n",
      "Epoch 7/100  Iteration 314/4600 Training loss: 2.3505 3.0731 sec/batch\n",
      "Epoch 7/100  Iteration 315/4600 Training loss: 2.3480 3.0852 sec/batch\n",
      "Epoch 7/100  Iteration 316/4600 Training loss: 2.3454 3.0968 sec/batch\n",
      "Epoch 7/100  Iteration 317/4600 Training loss: 2.3441 3.0964 sec/batch\n",
      "Epoch 7/100  Iteration 318/4600 Training loss: 2.3425 3.0698 sec/batch\n",
      "Epoch 7/100  Iteration 319/4600 Training loss: 2.3409 3.0824 sec/batch\n",
      "Epoch 7/100  Iteration 320/4600 Training loss: 2.3390 3.0797 sec/batch\n",
      "Epoch 7/100  Iteration 321/4600 Training loss: 2.3381 3.0747 sec/batch\n",
      "Epoch 7/100  Iteration 322/4600 Training loss: 2.3363 3.0746 sec/batch\n",
      "Epoch 8/100  Iteration 323/4600 Training loss: 2.3460 3.0970 sec/batch\n",
      "Epoch 8/100  Iteration 324/4600 Training loss: 2.3254 3.1430 sec/batch\n",
      "Epoch 8/100  Iteration 325/4600 Training loss: 2.3151 3.0861 sec/batch\n",
      "Epoch 8/100  Iteration 326/4600 Training loss: 2.2971 3.0973 sec/batch\n",
      "Epoch 8/100  Iteration 327/4600 Training loss: 2.2744 3.0936 sec/batch\n",
      "Epoch 8/100  Iteration 328/4600 Training loss: 2.2648 3.0848 sec/batch\n",
      "Epoch 8/100  Iteration 329/4600 Training loss: 2.2612 3.0757 sec/batch\n",
      "Epoch 8/100  Iteration 330/4600 Training loss: 2.2545 3.1010 sec/batch\n",
      "Epoch 8/100  Iteration 331/4600 Training loss: 2.2452 3.0731 sec/batch\n",
      "Epoch 8/100  Iteration 332/4600 Training loss: 2.2389 3.0931 sec/batch\n",
      "Epoch 8/100  Iteration 333/4600 Training loss: 2.2302 3.1004 sec/batch\n",
      "Epoch 8/100  Iteration 334/4600 Training loss: 2.2238 3.0895 sec/batch\n",
      "Epoch 8/100  Iteration 335/4600 Training loss: 2.2155 3.0757 sec/batch\n",
      "Epoch 8/100  Iteration 336/4600 Training loss: 2.2156 3.1017 sec/batch\n",
      "Epoch 8/100  Iteration 337/4600 Training loss: 2.2137 3.0768 sec/batch\n",
      "Epoch 8/100  Iteration 338/4600 Training loss: 2.2111 3.0775 sec/batch\n",
      "Epoch 8/100  Iteration 339/4600 Training loss: 2.2090 3.0852 sec/batch\n",
      "Epoch 8/100  Iteration 340/4600 Training loss: 2.2086 3.1083 sec/batch\n",
      "Epoch 8/100  Iteration 341/4600 Training loss: 2.2077 3.0964 sec/batch\n",
      "Epoch 8/100  Iteration 342/4600 Training loss: 2.2053 3.0949 sec/batch\n",
      "Epoch 8/100  Iteration 343/4600 Training loss: 2.2058 3.0894 sec/batch\n",
      "Epoch 8/100  Iteration 344/4600 Training loss: 2.2048 3.0934 sec/batch\n",
      "Epoch 8/100  Iteration 345/4600 Training loss: 2.2057 3.0943 sec/batch\n",
      "Epoch 8/100  Iteration 346/4600 Training loss: 2.2029 3.0918 sec/batch\n",
      "Epoch 8/100  Iteration 347/4600 Training loss: 2.2026 3.1032 sec/batch\n",
      "Epoch 8/100  Iteration 348/4600 Training loss: 2.2006 3.1392 sec/batch\n",
      "Epoch 8/100  Iteration 349/4600 Training loss: 2.1993 3.1244 sec/batch\n",
      "Epoch 8/100  Iteration 350/4600 Training loss: 2.1971 3.1135 sec/batch\n",
      "Epoch 8/100  Iteration 351/4600 Training loss: 2.1965 3.1045 sec/batch\n",
      "Epoch 8/100  Iteration 352/4600 Training loss: 2.1943 3.0768 sec/batch\n",
      "Epoch 8/100  Iteration 353/4600 Training loss: 2.1927 3.0750 sec/batch\n",
      "Epoch 8/100  Iteration 354/4600 Training loss: 2.1915 3.0904 sec/batch\n",
      "Epoch 8/100  Iteration 355/4600 Training loss: 2.1887 3.1175 sec/batch\n",
      "Epoch 8/100  Iteration 356/4600 Training loss: 2.1853 3.1488 sec/batch\n",
      "Epoch 8/100  Iteration 357/4600 Training loss: 2.1822 3.1288 sec/batch\n",
      "Epoch 8/100  Iteration 358/4600 Training loss: 2.1804 3.1503 sec/batch\n",
      "Epoch 8/100  Iteration 359/4600 Training loss: 2.1777 3.1170 sec/batch\n",
      "Epoch 8/100  Iteration 360/4600 Training loss: 2.1764 3.0937 sec/batch\n",
      "Epoch 8/100  Iteration 361/4600 Training loss: 2.1738 3.0871 sec/batch\n",
      "Epoch 8/100  Iteration 362/4600 Training loss: 2.1717 3.1160 sec/batch\n",
      "Epoch 8/100  Iteration 363/4600 Training loss: 2.1709 3.1036 sec/batch\n",
      "Epoch 8/100  Iteration 364/4600 Training loss: 2.1697 3.1034 sec/batch\n",
      "Epoch 8/100  Iteration 365/4600 Training loss: 2.1690 3.0996 sec/batch\n",
      "Epoch 8/100  Iteration 366/4600 Training loss: 2.1677 3.1122 sec/batch\n",
      "Epoch 8/100  Iteration 367/4600 Training loss: 2.1672 3.1256 sec/batch\n",
      "Epoch 8/100  Iteration 368/4600 Training loss: 2.1656 3.1349 sec/batch\n",
      "Epoch 9/100  Iteration 369/4600 Training loss: 2.2385 3.1063 sec/batch\n",
      "Epoch 9/100  Iteration 370/4600 Training loss: 2.1971 3.0831 sec/batch\n",
      "Epoch 9/100  Iteration 371/4600 Training loss: 2.1818 3.1030 sec/batch\n",
      "Epoch 9/100  Iteration 372/4600 Training loss: 2.1630 3.1054 sec/batch\n",
      "Epoch 9/100  Iteration 373/4600 Training loss: 2.1395 3.1193 sec/batch\n",
      "Epoch 9/100  Iteration 374/4600 Training loss: 2.1272 3.0859 sec/batch\n",
      "Epoch 9/100  Iteration 375/4600 Training loss: 2.1221 3.0883 sec/batch\n",
      "Epoch 9/100  Iteration 376/4600 Training loss: 2.1145 3.0727 sec/batch\n",
      "Epoch 9/100  Iteration 377/4600 Training loss: 2.1029 3.0987 sec/batch\n",
      "Epoch 9/100  Iteration 378/4600 Training loss: 2.0997 3.0820 sec/batch\n",
      "Epoch 9/100  Iteration 379/4600 Training loss: 2.0910 3.0975 sec/batch\n",
      "Epoch 9/100  Iteration 380/4600 Training loss: 2.0847 3.0525 sec/batch\n",
      "Epoch 9/100  Iteration 381/4600 Training loss: 2.0762 3.0577 sec/batch\n",
      "Epoch 9/100  Iteration 382/4600 Training loss: 2.0753 3.0983 sec/batch\n",
      "Epoch 9/100  Iteration 383/4600 Training loss: 2.0723 3.0985 sec/batch\n",
      "Epoch 9/100  Iteration 384/4600 Training loss: 2.0688 3.0847 sec/batch\n",
      "Epoch 9/100  Iteration 385/4600 Training loss: 2.0659 3.0727 sec/batch\n",
      "Epoch 9/100  Iteration 386/4600 Training loss: 2.0648 3.0988 sec/batch\n",
      "Epoch 9/100  Iteration 387/4600 Training loss: 2.0625 3.1236 sec/batch\n",
      "Epoch 9/100  Iteration 388/4600 Training loss: 2.0582 3.0772 sec/batch\n",
      "Epoch 9/100  Iteration 389/4600 Training loss: 2.0566 3.0684 sec/batch\n",
      "Epoch 9/100  Iteration 390/4600 Training loss: 2.0554 3.0727 sec/batch\n",
      "Epoch 9/100  Iteration 391/4600 Training loss: 2.0559 3.0667 sec/batch\n",
      "Epoch 9/100  Iteration 392/4600 Training loss: 2.0519 3.0720 sec/batch\n",
      "Epoch 9/100  Iteration 393/4600 Training loss: 2.0505 3.0994 sec/batch\n",
      "Epoch 9/100  Iteration 394/4600 Training loss: 2.0477 3.0946 sec/batch\n",
      "Epoch 9/100  Iteration 395/4600 Training loss: 2.0458 3.0882 sec/batch\n",
      "Epoch 9/100  Iteration 396/4600 Training loss: 2.0433 3.0810 sec/batch\n",
      "Epoch 9/100  Iteration 397/4600 Training loss: 2.0420 3.0985 sec/batch\n",
      "Epoch 9/100  Iteration 398/4600 Training loss: 2.0395 3.0984 sec/batch\n",
      "Epoch 9/100  Iteration 399/4600 Training loss: 2.0377 3.0767 sec/batch\n",
      "Epoch 9/100  Iteration 400/4600 Training loss: 2.0355 3.0762 sec/batch\n",
      "Validation loss: 1.93108 Saving checkpoint!\n",
      "Epoch 9/100  Iteration 401/4600 Training loss: 2.0373 3.1083 sec/batch\n",
      "Epoch 9/100  Iteration 402/4600 Training loss: 2.0338 3.1158 sec/batch\n",
      "Epoch 9/100  Iteration 403/4600 Training loss: 2.0305 3.0869 sec/batch\n",
      "Epoch 9/100  Iteration 404/4600 Training loss: 2.0288 3.0920 sec/batch\n",
      "Epoch 9/100  Iteration 405/4600 Training loss: 2.0256 3.0836 sec/batch\n",
      "Epoch 9/100  Iteration 406/4600 Training loss: 2.0244 3.0789 sec/batch\n",
      "Epoch 9/100  Iteration 407/4600 Training loss: 2.0221 3.1073 sec/batch\n",
      "Epoch 9/100  Iteration 408/4600 Training loss: 2.0199 3.0833 sec/batch\n",
      "Epoch 9/100  Iteration 409/4600 Training loss: 2.0192 3.0992 sec/batch\n",
      "Epoch 9/100  Iteration 410/4600 Training loss: 2.0185 3.1003 sec/batch\n",
      "Epoch 9/100  Iteration 411/4600 Training loss: 2.0180 3.0949 sec/batch\n",
      "Epoch 9/100  Iteration 412/4600 Training loss: 2.0169 3.0599 sec/batch\n",
      "Epoch 9/100  Iteration 413/4600 Training loss: 2.0165 3.0788 sec/batch\n",
      "Epoch 9/100  Iteration 414/4600 Training loss: 2.0152 3.0857 sec/batch\n",
      "Epoch 10/100  Iteration 415/4600 Training loss: 2.0944 3.0776 sec/batch\n",
      "Epoch 10/100  Iteration 416/4600 Training loss: 2.0582 3.0744 sec/batch\n",
      "Epoch 10/100  Iteration 417/4600 Training loss: 2.0431 3.0900 sec/batch\n",
      "Epoch 10/100  Iteration 418/4600 Training loss: 2.0198 3.1084 sec/batch\n",
      "Epoch 10/100  Iteration 419/4600 Training loss: 1.9935 3.0839 sec/batch\n",
      "Epoch 10/100  Iteration 420/4600 Training loss: 1.9791 3.0820 sec/batch\n",
      "Epoch 10/100  Iteration 421/4600 Training loss: 1.9698 3.0794 sec/batch\n",
      "Epoch 10/100  Iteration 422/4600 Training loss: 1.9590 3.0845 sec/batch\n",
      "Epoch 10/100  Iteration 423/4600 Training loss: 1.9468 3.0799 sec/batch\n",
      "Epoch 10/100  Iteration 424/4600 Training loss: 1.9412 3.0760 sec/batch\n",
      "Epoch 10/100  Iteration 425/4600 Training loss: 1.9337 3.0677 sec/batch\n",
      "Epoch 10/100  Iteration 426/4600 Training loss: 1.9294 3.0774 sec/batch\n",
      "Epoch 10/100  Iteration 427/4600 Training loss: 1.9224 3.1036 sec/batch\n",
      "Epoch 10/100  Iteration 428/4600 Training loss: 1.9215 3.0868 sec/batch\n",
      "Epoch 10/100  Iteration 429/4600 Training loss: 1.9183 3.1014 sec/batch\n",
      "Epoch 10/100  Iteration 430/4600 Training loss: 1.9130 3.0850 sec/batch\n",
      "Epoch 10/100  Iteration 431/4600 Training loss: 1.9102 3.0618 sec/batch\n",
      "Epoch 10/100  Iteration 432/4600 Training loss: 1.9089 3.0845 sec/batch\n",
      "Epoch 10/100  Iteration 433/4600 Training loss: 1.9073 3.1048 sec/batch\n",
      "Epoch 10/100  Iteration 434/4600 Training loss: 1.9028 3.1146 sec/batch\n",
      "Epoch 10/100  Iteration 435/4600 Training loss: 1.9012 3.1260 sec/batch\n",
      "Epoch 10/100  Iteration 436/4600 Training loss: 1.8998 3.0821 sec/batch\n",
      "Epoch 10/100  Iteration 437/4600 Training loss: 1.9001 3.0794 sec/batch\n",
      "Epoch 10/100  Iteration 438/4600 Training loss: 1.8968 3.0822 sec/batch\n",
      "Epoch 10/100  Iteration 439/4600 Training loss: 1.8952 3.0776 sec/batch\n",
      "Epoch 10/100  Iteration 440/4600 Training loss: 1.8928 3.0684 sec/batch\n",
      "Epoch 10/100  Iteration 441/4600 Training loss: 1.8904 3.0998 sec/batch\n",
      "Epoch 10/100  Iteration 442/4600 Training loss: 1.8877 3.1109 sec/batch\n",
      "Epoch 10/100  Iteration 443/4600 Training loss: 1.8858 3.0905 sec/batch\n",
      "Epoch 10/100  Iteration 444/4600 Training loss: 1.8826 3.1080 sec/batch\n",
      "Epoch 10/100  Iteration 445/4600 Training loss: 1.8802 3.0973 sec/batch\n",
      "Epoch 10/100  Iteration 446/4600 Training loss: 1.8779 3.1026 sec/batch\n",
      "Epoch 10/100  Iteration 447/4600 Training loss: 1.8751 3.0877 sec/batch\n",
      "Epoch 10/100  Iteration 448/4600 Training loss: 1.8718 3.0815 sec/batch\n",
      "Epoch 10/100  Iteration 449/4600 Training loss: 1.8684 3.1293 sec/batch\n",
      "Epoch 10/100  Iteration 450/4600 Training loss: 1.8661 3.1097 sec/batch\n",
      "Epoch 10/100  Iteration 451/4600 Training loss: 1.8636 3.1351 sec/batch\n",
      "Epoch 10/100  Iteration 452/4600 Training loss: 1.8627 3.1018 sec/batch\n",
      "Epoch 10/100  Iteration 453/4600 Training loss: 1.8608 3.1025 sec/batch\n",
      "Epoch 10/100  Iteration 454/4600 Training loss: 1.8592 3.0948 sec/batch\n",
      "Epoch 10/100  Iteration 455/4600 Training loss: 1.8589 3.1217 sec/batch\n",
      "Epoch 10/100  Iteration 456/4600 Training loss: 1.8590 3.1251 sec/batch\n",
      "Epoch 10/100  Iteration 457/4600 Training loss: 1.8589 3.1314 sec/batch\n",
      "Epoch 10/100  Iteration 458/4600 Training loss: 1.8582 3.0974 sec/batch\n",
      "Epoch 10/100  Iteration 459/4600 Training loss: 1.8580 3.0860 sec/batch\n",
      "Epoch 10/100  Iteration 460/4600 Training loss: 1.8572 3.0840 sec/batch\n",
      "Epoch 11/100  Iteration 461/4600 Training loss: 1.9824 3.0748 sec/batch\n",
      "Epoch 11/100  Iteration 462/4600 Training loss: 1.9381 3.0676 sec/batch\n",
      "Epoch 11/100  Iteration 463/4600 Training loss: 1.9143 3.0733 sec/batch\n",
      "Epoch 11/100  Iteration 464/4600 Training loss: 1.8879 3.0625 sec/batch\n",
      "Epoch 11/100  Iteration 465/4600 Training loss: 1.8606 3.0717 sec/batch\n",
      "Epoch 11/100  Iteration 466/4600 Training loss: 1.8427 3.0676 sec/batch\n",
      "Epoch 11/100  Iteration 467/4600 Training loss: 1.8324 3.0535 sec/batch\n",
      "Epoch 11/100  Iteration 468/4600 Training loss: 1.8204 3.0619 sec/batch\n",
      "Epoch 11/100  Iteration 469/4600 Training loss: 1.8104 3.1202 sec/batch\n",
      "Epoch 11/100  Iteration 470/4600 Training loss: 1.8048 3.0898 sec/batch\n",
      "Epoch 11/100  Iteration 471/4600 Training loss: 1.7984 3.0709 sec/batch\n",
      "Epoch 11/100  Iteration 472/4600 Training loss: 1.7960 3.1109 sec/batch\n",
      "Epoch 11/100  Iteration 473/4600 Training loss: 1.7896 3.1074 sec/batch\n",
      "Epoch 11/100  Iteration 474/4600 Training loss: 1.7896 3.0881 sec/batch\n",
      "Epoch 11/100  Iteration 475/4600 Training loss: 1.7867 3.0923 sec/batch\n",
      "Epoch 11/100  Iteration 476/4600 Training loss: 1.7825 3.0605 sec/batch\n",
      "Epoch 11/100  Iteration 477/4600 Training loss: 1.7796 3.1364 sec/batch\n",
      "Epoch 11/100  Iteration 478/4600 Training loss: 1.7786 3.1173 sec/batch\n",
      "Epoch 11/100  Iteration 479/4600 Training loss: 1.7766 3.1150 sec/batch\n",
      "Epoch 11/100  Iteration 480/4600 Training loss: 1.7730 3.0782 sec/batch\n",
      "Epoch 11/100  Iteration 481/4600 Training loss: 1.7709 3.0718 sec/batch\n",
      "Epoch 11/100  Iteration 482/4600 Training loss: 1.7701 3.0758 sec/batch\n",
      "Epoch 11/100  Iteration 483/4600 Training loss: 1.7706 3.1080 sec/batch\n",
      "Epoch 11/100  Iteration 484/4600 Training loss: 1.7673 3.0763 sec/batch\n",
      "Epoch 11/100  Iteration 485/4600 Training loss: 1.7649 3.1037 sec/batch\n",
      "Epoch 11/100  Iteration 486/4600 Training loss: 1.7616 3.0827 sec/batch\n",
      "Epoch 11/100  Iteration 487/4600 Training loss: 1.7593 3.0843 sec/batch\n",
      "Epoch 11/100  Iteration 488/4600 Training loss: 1.7566 3.0574 sec/batch\n",
      "Epoch 11/100  Iteration 489/4600 Training loss: 1.7543 3.0759 sec/batch\n",
      "Epoch 11/100  Iteration 490/4600 Training loss: 1.7506 3.1332 sec/batch\n",
      "Epoch 11/100  Iteration 491/4600 Training loss: 1.7477 3.1000 sec/batch\n",
      "Epoch 11/100  Iteration 492/4600 Training loss: 1.7456 3.0672 sec/batch\n",
      "Epoch 11/100  Iteration 493/4600 Training loss: 1.7423 3.0850 sec/batch\n",
      "Epoch 11/100  Iteration 494/4600 Training loss: 1.7383 3.0866 sec/batch\n",
      "Epoch 11/100  Iteration 495/4600 Training loss: 1.7346 3.0904 sec/batch\n",
      "Epoch 11/100  Iteration 496/4600 Training loss: 1.7318 3.0806 sec/batch\n",
      "Epoch 11/100  Iteration 497/4600 Training loss: 1.7291 3.0807 sec/batch\n",
      "Epoch 11/100  Iteration 498/4600 Training loss: 1.7279 3.0752 sec/batch\n",
      "Epoch 11/100  Iteration 499/4600 Training loss: 1.7261 3.0942 sec/batch\n",
      "Epoch 11/100  Iteration 500/4600 Training loss: 1.7246 3.0911 sec/batch\n",
      "Epoch 11/100  Iteration 501/4600 Training loss: 1.7248 3.0567 sec/batch\n",
      "Epoch 11/100  Iteration 502/4600 Training loss: 1.7254 3.1066 sec/batch\n",
      "Epoch 11/100  Iteration 503/4600 Training loss: 1.7253 3.0986 sec/batch\n",
      "Epoch 11/100  Iteration 504/4600 Training loss: 1.7246 3.0627 sec/batch\n",
      "Epoch 11/100  Iteration 505/4600 Training loss: 1.7247 3.1072 sec/batch\n",
      "Epoch 11/100  Iteration 506/4600 Training loss: 1.7240 3.0814 sec/batch\n",
      "Epoch 12/100  Iteration 507/4600 Training loss: 1.8791 3.0868 sec/batch\n",
      "Epoch 12/100  Iteration 508/4600 Training loss: 1.8182 3.0785 sec/batch\n",
      "Epoch 12/100  Iteration 509/4600 Training loss: 1.7953 3.0736 sec/batch\n",
      "Epoch 12/100  Iteration 510/4600 Training loss: 1.7625 3.0839 sec/batch\n",
      "Epoch 12/100  Iteration 511/4600 Training loss: 1.7347 3.1085 sec/batch\n",
      "Epoch 12/100  Iteration 512/4600 Training loss: 1.7129 3.0816 sec/batch\n",
      "Epoch 12/100  Iteration 513/4600 Training loss: 1.6985 3.0710 sec/batch\n",
      "Epoch 12/100  Iteration 514/4600 Training loss: 1.6835 3.1167 sec/batch\n",
      "Epoch 12/100  Iteration 515/4600 Training loss: 1.6725 3.0910 sec/batch\n",
      "Epoch 12/100  Iteration 516/4600 Training loss: 1.6681 3.0957 sec/batch\n",
      "Epoch 12/100  Iteration 517/4600 Training loss: 1.6641 3.0727 sec/batch\n",
      "Epoch 12/100  Iteration 518/4600 Training loss: 1.6625 3.0767 sec/batch\n",
      "Epoch 12/100  Iteration 519/4600 Training loss: 1.6572 3.0739 sec/batch\n",
      "Epoch 12/100  Iteration 520/4600 Training loss: 1.6569 3.0643 sec/batch\n",
      "Epoch 12/100  Iteration 521/4600 Training loss: 1.6545 3.0991 sec/batch\n",
      "Epoch 12/100  Iteration 522/4600 Training loss: 1.6503 3.0942 sec/batch\n",
      "Epoch 12/100  Iteration 523/4600 Training loss: 1.6473 3.0945 sec/batch\n",
      "Epoch 12/100  Iteration 524/4600 Training loss: 1.6463 3.0738 sec/batch\n",
      "Epoch 12/100  Iteration 525/4600 Training loss: 1.6445 3.0773 sec/batch\n",
      "Epoch 12/100  Iteration 526/4600 Training loss: 1.6405 3.0708 sec/batch\n",
      "Epoch 12/100  Iteration 527/4600 Training loss: 1.6388 3.0957 sec/batch\n",
      "Epoch 12/100  Iteration 528/4600 Training loss: 1.6371 3.0858 sec/batch\n",
      "Epoch 12/100  Iteration 529/4600 Training loss: 1.6374 3.0681 sec/batch\n",
      "Epoch 12/100  Iteration 530/4600 Training loss: 1.6333 3.1004 sec/batch\n",
      "Epoch 12/100  Iteration 531/4600 Training loss: 1.6308 3.0636 sec/batch\n",
      "Epoch 12/100  Iteration 532/4600 Training loss: 1.6278 3.1010 sec/batch\n",
      "Epoch 12/100  Iteration 533/4600 Training loss: 1.6250 3.0921 sec/batch\n",
      "Epoch 12/100  Iteration 534/4600 Training loss: 1.6223 3.0843 sec/batch\n",
      "Epoch 12/100  Iteration 535/4600 Training loss: 1.6197 3.1205 sec/batch\n",
      "Epoch 12/100  Iteration 536/4600 Training loss: 1.6162 3.0616 sec/batch\n",
      "Epoch 12/100  Iteration 537/4600 Training loss: 1.6138 3.0835 sec/batch\n",
      "Epoch 12/100  Iteration 538/4600 Training loss: 1.6113 3.1089 sec/batch\n",
      "Epoch 12/100  Iteration 539/4600 Training loss: 1.6084 3.0853 sec/batch\n",
      "Epoch 12/100  Iteration 540/4600 Training loss: 1.6044 3.0724 sec/batch\n",
      "Epoch 12/100  Iteration 541/4600 Training loss: 1.6007 3.0661 sec/batch\n",
      "Epoch 12/100  Iteration 542/4600 Training loss: 1.5984 3.0844 sec/batch\n",
      "Epoch 12/100  Iteration 543/4600 Training loss: 1.5959 3.0973 sec/batch\n",
      "Epoch 12/100  Iteration 544/4600 Training loss: 1.5952 3.0660 sec/batch\n",
      "Epoch 12/100  Iteration 545/4600 Training loss: 1.5937 3.0877 sec/batch\n",
      "Epoch 12/100  Iteration 546/4600 Training loss: 1.5924 3.1080 sec/batch\n",
      "Epoch 12/100  Iteration 547/4600 Training loss: 1.5928 3.0782 sec/batch\n",
      "Epoch 12/100  Iteration 548/4600 Training loss: 1.5939 3.0818 sec/batch\n",
      "Epoch 12/100  Iteration 549/4600 Training loss: 1.5945 3.1140 sec/batch\n",
      "Epoch 12/100  Iteration 550/4600 Training loss: 1.5946 3.1662 sec/batch\n",
      "Epoch 12/100  Iteration 551/4600 Training loss: 1.5949 3.1099 sec/batch\n",
      "Epoch 12/100  Iteration 552/4600 Training loss: 1.5949 3.1031 sec/batch\n",
      "Epoch 13/100  Iteration 553/4600 Training loss: 1.7881 3.0981 sec/batch\n",
      "Epoch 13/100  Iteration 554/4600 Training loss: 1.7093 3.1584 sec/batch\n",
      "Epoch 13/100  Iteration 555/4600 Training loss: 1.6900 3.1524 sec/batch\n",
      "Epoch 13/100  Iteration 556/4600 Training loss: 1.6628 3.1081 sec/batch\n",
      "Epoch 13/100  Iteration 557/4600 Training loss: 1.6282 3.1059 sec/batch\n",
      "Epoch 13/100  Iteration 558/4600 Training loss: 1.6079 3.0735 sec/batch\n",
      "Epoch 13/100  Iteration 559/4600 Training loss: 1.5927 3.0901 sec/batch\n",
      "Epoch 13/100  Iteration 560/4600 Training loss: 1.5764 3.0784 sec/batch\n",
      "Epoch 13/100  Iteration 561/4600 Training loss: 1.5659 3.0778 sec/batch\n",
      "Epoch 13/100  Iteration 562/4600 Training loss: 1.5623 3.0852 sec/batch\n",
      "Epoch 13/100  Iteration 563/4600 Training loss: 1.5591 3.1097 sec/batch\n",
      "Epoch 13/100  Iteration 564/4600 Training loss: 1.5588 3.0983 sec/batch\n",
      "Epoch 13/100  Iteration 565/4600 Training loss: 1.5546 3.1223 sec/batch\n",
      "Epoch 13/100  Iteration 566/4600 Training loss: 1.5557 3.0808 sec/batch\n",
      "Epoch 13/100  Iteration 567/4600 Training loss: 1.5542 3.0791 sec/batch\n",
      "Epoch 13/100  Iteration 568/4600 Training loss: 1.5486 3.1108 sec/batch\n",
      "Epoch 13/100  Iteration 569/4600 Training loss: 1.5454 3.1055 sec/batch\n",
      "Epoch 13/100  Iteration 570/4600 Training loss: 1.5448 3.0690 sec/batch\n",
      "Epoch 13/100  Iteration 571/4600 Training loss: 1.5425 3.0651 sec/batch\n",
      "Epoch 13/100  Iteration 572/4600 Training loss: 1.5387 3.0833 sec/batch\n",
      "Epoch 13/100  Iteration 573/4600 Training loss: 1.5360 3.0916 sec/batch\n",
      "Epoch 13/100  Iteration 574/4600 Training loss: 1.5339 3.2167 sec/batch\n",
      "Epoch 13/100  Iteration 575/4600 Training loss: 1.5342 3.1693 sec/batch\n",
      "Epoch 13/100  Iteration 576/4600 Training loss: 1.5303 3.1262 sec/batch\n",
      "Epoch 13/100  Iteration 577/4600 Training loss: 1.5275 3.1484 sec/batch\n",
      "Epoch 13/100  Iteration 578/4600 Training loss: 1.5250 3.1719 sec/batch\n",
      "Epoch 13/100  Iteration 579/4600 Training loss: 1.5225 3.1644 sec/batch\n",
      "Epoch 13/100  Iteration 580/4600 Training loss: 1.5201 3.1951 sec/batch\n",
      "Epoch 13/100  Iteration 581/4600 Training loss: 1.5178 3.1828 sec/batch\n",
      "Epoch 13/100  Iteration 582/4600 Training loss: 1.5143 3.1372 sec/batch\n",
      "Epoch 13/100  Iteration 583/4600 Training loss: 1.5116 3.1529 sec/batch\n",
      "Epoch 13/100  Iteration 584/4600 Training loss: 1.5089 3.1870 sec/batch\n",
      "Epoch 13/100  Iteration 585/4600 Training loss: 1.5060 3.1068 sec/batch\n",
      "Epoch 13/100  Iteration 586/4600 Training loss: 1.5025 3.1431 sec/batch\n",
      "Epoch 13/100  Iteration 587/4600 Training loss: 1.4991 3.2356 sec/batch\n",
      "Epoch 13/100  Iteration 588/4600 Training loss: 1.4965 3.1180 sec/batch\n",
      "Epoch 13/100  Iteration 589/4600 Training loss: 1.4941 3.1039 sec/batch\n",
      "Epoch 13/100  Iteration 590/4600 Training loss: 1.4931 3.0823 sec/batch\n",
      "Epoch 13/100  Iteration 591/4600 Training loss: 1.4920 3.1028 sec/batch\n",
      "Epoch 13/100  Iteration 592/4600 Training loss: 1.4913 3.0957 sec/batch\n",
      "Epoch 13/100  Iteration 593/4600 Training loss: 1.4920 3.0873 sec/batch\n",
      "Epoch 13/100  Iteration 594/4600 Training loss: 1.4933 3.0984 sec/batch\n",
      "Epoch 13/100  Iteration 595/4600 Training loss: 1.4941 3.1125 sec/batch\n",
      "Epoch 13/100  Iteration 596/4600 Training loss: 1.4942 3.1308 sec/batch\n",
      "Epoch 13/100  Iteration 597/4600 Training loss: 1.4947 3.1158 sec/batch\n",
      "Epoch 13/100  Iteration 598/4600 Training loss: 1.4948 3.1053 sec/batch\n",
      "Epoch 14/100  Iteration 599/4600 Training loss: 1.7147 3.0948 sec/batch\n",
      "Epoch 14/100  Iteration 600/4600 Training loss: 1.6261 3.0686 sec/batch\n",
      "Validation loss: 1.44013 Saving checkpoint!\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "checkpoints/i600_l512_v1.440.ckpt.meta.tmpcf3b11eaa88f4a2dad478f4aab93858e",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-1b5aa284b859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m                 print('Validation loss:', np.mean(val_loss),\n\u001b[1;32m     57\u001b[0m                       'Saving checkpoint!')\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"checkpoints/i{}_l{}_v{:.3f}.ckpt\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1373\u001b[0m           checkpoint_file, meta_graph_suffix=meta_graph_suffix)\n\u001b[1;32m   1374\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text, export_scope, clear_devices)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mas_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m         \u001b[0mexport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m         clear_devices=clear_devices)\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text, graph, export_scope, clear_devices, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m       \u001b[0mexport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m       \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m   1631\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mexport_scoped_meta_graph\u001b[0;34m(filename, graph_def, graph, export_scope, as_text, unbound_inputs_col_name, clear_devices, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         as_text=as_text)\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mscoped_meta_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/framework/graph_io.py\u001b[0m in \u001b[0;36mwrite_graph\u001b[0;34m(graph_or_graph_def, logdir, name, as_text)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matomic_write_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matomic_write_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36matomic_write_string_to_file\u001b[0;34m(filename, contents)\u001b[0m\n\u001b[1;32m    350\u001b[0m   \"\"\"\n\u001b[1;32m    351\u001b[0m   \u001b[0mtemp_pathname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".tmp\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m   \u001b[0mwrite_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m   \u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mwrite_string_to_file\u001b[0;34m(filename, file_content)\u001b[0m\n\u001b[1;32m    247\u001b[0m   \"\"\"\n\u001b[1;32m    248\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, file_content)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       pywrap_tensorflow.AppendToFile(\n\u001b[0;32m---> 93\u001b[0;31m           compat.as_bytes(file_content), self._writable_file, status)\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    464\u001b[0m           \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteStatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: checkpoints/i600_l512_v1.440.ckpt.meta.tmpcf3b11eaa88f4a2dad478f4aab93858e"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"public \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public talan an  of thuring, tentror as = nen = tetrate ()\n",
      "     }\n",
      "\n",
      " }\n",
      "\n",
      "\t***\n",
      "\t***\n",
      " * Thic shices.s\n",
      "\t*\n",
      " **\n",
      "\t * <!-- unteunser-doc -->\n",
      " *\n",
      "\t*\n",
      " * * @etrint\n",
      " *\n",
      " */ That * sithe thes cempliane ting chetens er font ind atilithe af.\n",
      "\t * @lenirt vaet art angablingt or ender thas terstibe or.gicilteon tor.\n",
      "\tputilad at erteraterithictond of tole cestics\n",
      "\t\t/\tpontre = bolals;\n",
      "\t\tfore torg catenes catary\n",
      " */\n",
      "\t /\n",
      " * <@ge> umget octest om entils at at of thinew or the tinte faraned. \n",
      "  @Tist\n",
      "\t * {\n",
      "\t * @ter ablectobl cotate chatinsertate = naterotare.dat(Daslecrate();;\n",
      "\t}//\n",
      "\t*/\n",
      "\t * @eter the fintar on tith te for the thor s ercins thetefred fhe.s tibul = bicting, asting = now Conlomet();\n",
      "\t\tfesit  {\n",
      "\n",
      "\t\t privile = teter.gictonder.etict());\n",
      "\t\tforane = bulle chonter = cemtol = falle.tomals.gater();\n",
      "\t\t\tfetatir orgetare = sace();\n",
      "\t\tif (aturate terale( endint = tort).entEdapteres);\n",
      "\n",
      "\t\t/@senirindESetangexdapter(();\n",
      "\t\tfalicas Contomere veel nectethed alalathicat = {\n",
      "\t\t/\n",
      "\t //\n",
      "\t/ @seretint.esterdEnclaterast());\n",
      "\t\tfor (atintor tore competite)\n",
      "\n",
      "\t\t panlecress.valedestExcangeretFeturtExsection());\n",
      "}\n",
      "\n",
      "\n",
      " **\n",
      "\n",
      "\t**************************************************************************************************************************************************************************************************************************\n",
      "******************************************************************************************************************************************************************************************************************************************************************** *\n",
      "***************************************************************************************************************************************************************************************************************************************************\n",
      "/**\n",
      " * * Thas Locathe(.S AS   ches andetaridite torit ath if the terew ang ons lonter the setroc anginse fiteritind adedath on worite to that ander the s cetas  feradict isges chote the tore \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/i920_l128_v1.989.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"public \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
