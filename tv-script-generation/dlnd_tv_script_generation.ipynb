{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TV Script Generation\n",
    "In this project, you'll generate your own [Simpsons](https://en.wikipedia.org/wiki/The_Simpsons) TV scripts using RNNs.  You'll be using part of the [Simpsons dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data) of scripts from 27 seasons.  The Neural Network you'll build will generate a new TV script for a scene at [Moe's Tavern](https://simpsonswiki.com/wiki/Moe's_Tavern).\n",
    "## Get the Data\n",
    "The data is already provided for you.  You'll be using a subset of the original dataset.  It consists of only the scenes in Moe's Tavern.  This doesn't include other versions of the tavern, like \"Moe's Cavern\", \"Flaming Moe's\", \"Uncle Moe's Family Feed-Bag\", etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "\n",
    "data_dir = './data/simpsons/moes_tavern_lines.txt'\n",
    "text = helper.load_data(data_dir)\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "text = text[81:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "Play around with `view_sentence_range` to view different parts of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 11492\n",
      "Number of scenes: 262\n",
      "Average number of sentences in each scene: 15.248091603053435\n",
      "Number of lines: 4257\n",
      "Average number of words in each line: 11.50434578341555\n",
      "\n",
      "The sentences 100 to 110:\n",
      "Barney_Gumble: Wow, it really works.\n",
      "HARV: (CHUCKLING) I'll be back.\n",
      "Homer_Simpson: Moe, I haven't seen the place this crowded since the government cracked down on you for accepting food stamps. Do you think my drink had something to do with it?\n",
      "Moe_Szyslak: Who can say? It's probably a combination of things.\n",
      "Patron_#1: (TO MOE) Another pitcher of those amazing \"Flaming Moe's\".\n",
      "Patron_#2: Boy, I hate this joint, but I love that drink.\n",
      "Collette: Barkeep, I couldn't help noticing your sign.\n",
      "Moe_Szyslak: The one that says, \"Bartenders Do It 'Til You Barf\"?\n",
      "Collette: No, above that store-bought drollery.\n",
      "Moe_Szyslak: Oh great! Why don't we fill out an application? (READING) I'll need your name, measurements and turn ons..\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (100, 110)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocessing Functions\n",
    "The first thing to do to any dataset is preprocessing.  Implement the following preprocessing functions below:\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following tuple `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    vocab_to_int = {}\n",
    "    int_to_vocab = {}\n",
    "    for index, word in enumerate(set(text)):\n",
    "        vocab_to_int[word] = index\n",
    "        int_to_vocab[index] = word\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word \"bye\" and \"bye!\".\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( . )\n",
    "- Comma ( , )\n",
    "- Quotation Mark ( \" )\n",
    "- Semicolon ( ; )\n",
    "- Exclamation mark ( ! )\n",
    "- Question mark ( ? )\n",
    "- Left Parentheses ( ( )\n",
    "- Right Parentheses ( ) )\n",
    "- Dash ( -- )\n",
    "- Return ( \\n )\n",
    "\n",
    "This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. Make sure you don't use a token that could be confused as a word. Instead of using the token \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    punctuation = {\n",
    "        '.': '||Period||',\n",
    "        ',': '||Comma||',\n",
    "        '\"': '||Quotation_Mark||',\n",
    "        ';': '||Semicolon||',\n",
    "        '!': '||Exclamation_Mark||',\n",
    "        '?': '||Question_Mark||',\n",
    "        '(': '||Left_Parentheses||',\n",
    "        ')': '||Right_Parentheses||',\n",
    "        '--': '||Dash||',\n",
    "        '\\n': '||Return||'}\n",
    "    \n",
    "    return punctuation\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the Neural Network\n",
    "You'll build the components necessary to build a RNN by implementing the following functions below:\n",
    "- get_inputs\n",
    "- get_init_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### Check the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Input\n",
    "Implement the `get_inputs()` function to create TF Placeholders for the Neural Network.  It should create the following placeholders:\n",
    "- Input text placeholder named \"input\" using the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) `name` parameter.\n",
    "- Targets placeholder\n",
    "- Learning Rate placeholder\n",
    "\n",
    "Return the placeholders in the following tuple `(Input, Targets, LearningRate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    return inputs_, targets, learning_rate\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build RNN Cell and Initialize\n",
    "Stack one or more [`BasicLSTMCells`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell) in a [`MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell).\n",
    "- The Rnn size should be set using `rnn_size`\n",
    "- Initalize Cell State using the MultiRNNCell's [`zero_state()`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state) function\n",
    "    - Apply the name \"initial_state\" to the initial state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the cell and initial state in the following tuple `(Cell, InitialState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    keep_prob = 0.7\n",
    "    lstm_layers = 1\n",
    "\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    dropout = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([dropout] * lstm_layers)\n",
    "    initial_state = tf.identity(cell.zero_state(batch_size, tf.float32), name='initial_state')\n",
    "    \n",
    "#     print(\"LSTM cell size: \", lstm.state_size)\n",
    "#     print(\"LSTM cell output size: \", lstm.output_size)\n",
    "#     print(\"LSTM layer size: \", cell.state_size)\n",
    "#     print(\"LSTM layer output size: \", cell.output_size)\n",
    "#     print('initial state size:', initial_state.shape)\n",
    "    \n",
    "    return cell, initial_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Word Embedding\n",
    "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embeddings = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "#     print('Embedding layer input size: ', input_data.shape)\n",
    "#     print(\"Embedding layer size: ({}, {})\".format(vocab_size, embed_dim))\n",
    "    return tf.nn.embedding_lookup(embeddings, input_data)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build RNN\n",
    "You created a RNN Cell in the `get_init_cell()` function.  Time to use the cell to create a RNN.\n",
    "- Build the RNN using the [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    " - Apply the name \"final_state\" to the final state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the outputs and final_state state in the following tuple `(Outputs, FinalState)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "#     print('output size:', outputs.shape)\n",
    "    return outputs, tf.identity(final_state, name='final_state')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "- Apply embedding to `input_data` using your `get_embed(input_data, vocab_size, embed_dim)` function.\n",
    "- Build RNN using `cell` and your `build_rnn(cell, inputs)` function.\n",
    "- Apply a fully connected layer with a linear activation and `vocab_size` as the number of outputs.\n",
    "\n",
    "Return the logits and final state in the following tuple (Logits, FinalState) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embeddings = get_embed(input_data, vocab_size, embed_dim)\n",
    "    outputs, final_state = build_rnn(cell, embeddings)\n",
    "#     print('output size:', outputs.shape)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "#     print('logits size:', logits.shape)\n",
    "    \n",
    "    return logits, final_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Batches\n",
    "Implement `get_batches` to create batches of input and targets using `int_text`.  The batches should be a Numpy array with the shape `(number of batches, 2, batch size, sequence length)`. Each batch contains two elements:\n",
    "- The first element is a single batch of **input** with the shape `[batch size, sequence length]`\n",
    "- The second element is a single batch of **targets** with the shape `[batch size, sequence length]`\n",
    "\n",
    "If you can't fill the last batch with enough data, drop the last batch.\n",
    "\n",
    "For exmple, `get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 2, 3)` would return a Numpy array of the following:\n",
    "```\n",
    "[\n",
    "  # First Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 1  2  3], [ 7  8  9]],\n",
    "    # Batch of targets\n",
    "    [[ 2  3  4], [ 8  9 10]]\n",
    "  ],\n",
    " \n",
    "  # Second Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 4  5  6], [10 11 12]],\n",
    "    # Batch of targets\n",
    "    [[ 5  6  7], [11 12 13]]\n",
    "  ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "#     print(\"input length: \", len(int_text))\n",
    "#     print('batch size: ', batch_size)\n",
    "#     print('sequence length: ', seq_length)\n",
    "    \n",
    "    num_batches = len(int_text)//(batch_size * seq_length)\n",
    "#     print('number of batches:', num_batches)\n",
    "    batch_length = num_batches * seq_length\n",
    "#     print('batch_length:', batch_length)\n",
    "    \n",
    "#     expected_batch_size = np.zeros((num_batches, 2, batch_size, seq_length)).shape\n",
    "    \n",
    "    batches = []\n",
    "    for i in range(num_batches):        \n",
    "        inputs = []\n",
    "        targets = []\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            cursor = i*num_batches + j*batch_length\n",
    "#         for k in range(batch_length):\n",
    "            x = int_text[cursor: (cursor + seq_length)]\n",
    "            y = int_text[(cursor + 1): (cursor + 1 + seq_length)]\n",
    "            inputs.append(x)\n",
    "            targets.append(y)\n",
    "        \n",
    "#         print('batch inputs:', inputs)\n",
    "#         print('batch targets:', targets)\n",
    "        batch = [inputs, targets]\n",
    "        batches.append(batch)        \n",
    "    \n",
    "#     print(np.array(batches).shape)\n",
    "#     print(expected_batch_size, '\\n')\n",
    "    return np.array(batches)\n",
    "\n",
    "def expected_batch_shape(input_size, batch_size, seq_length):\n",
    "    num_batches = input_size//(batch_size * seq_length)\n",
    "    return np.zeros((num_batches, 2, batch_size, seq_length)).shape \n",
    "    \n",
    "assert(get_batches([i+1 for i in range(15)], 2, 3).shape == expected_batch_shape(15,2,3))\n",
    "assert(get_batches([i+1 for i in range(19)], 2, 3).shape == expected_batch_shape(19,2,3))\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `num_epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `embed_dim` to the size of the embedding.\n",
    "- Set `seq_length` to the length of sequence.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `show_every_n_batches` to the number of batches the neural network should print progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 1500\n",
    "\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "\n",
    "# RNN Size\n",
    "rnn_size = 200\n",
    "\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 100\n",
    "\n",
    "# Sequence Length\n",
    "seq_length = 10\n",
    "\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 64\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data.  If you have a hard time getting a good loss, check the [forms](https://discussions.udacity.com/) to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/26   train_loss = 8.823\n",
      "Epoch   2 Batch   12/26   train_loss = 6.059\n",
      "Epoch   4 Batch   24/26   train_loss = 5.660\n",
      "Epoch   7 Batch   10/26   train_loss = 5.480\n",
      "Epoch   9 Batch   22/26   train_loss = 5.181\n",
      "Epoch  12 Batch    8/26   train_loss = 5.175\n",
      "Epoch  14 Batch   20/26   train_loss = 4.807\n",
      "Epoch  17 Batch    6/26   train_loss = 4.698\n",
      "Epoch  19 Batch   18/26   train_loss = 4.591\n",
      "Epoch  22 Batch    4/26   train_loss = 4.180\n",
      "Epoch  24 Batch   16/26   train_loss = 4.261\n",
      "Epoch  27 Batch    2/26   train_loss = 3.885\n",
      "Epoch  29 Batch   14/26   train_loss = 3.752\n",
      "Epoch  32 Batch    0/26   train_loss = 3.635\n",
      "Epoch  34 Batch   12/26   train_loss = 3.489\n",
      "Epoch  36 Batch   24/26   train_loss = 3.395\n",
      "Epoch  39 Batch   10/26   train_loss = 3.252\n",
      "Epoch  41 Batch   22/26   train_loss = 3.143\n",
      "Epoch  44 Batch    8/26   train_loss = 3.461\n",
      "Epoch  46 Batch   20/26   train_loss = 2.878\n",
      "Epoch  49 Batch    6/26   train_loss = 3.150\n",
      "Epoch  51 Batch   18/26   train_loss = 3.145\n",
      "Epoch  54 Batch    4/26   train_loss = 2.624\n",
      "Epoch  56 Batch   16/26   train_loss = 2.840\n",
      "Epoch  59 Batch    2/26   train_loss = 2.444\n",
      "Epoch  61 Batch   14/26   train_loss = 2.364\n",
      "Epoch  64 Batch    0/26   train_loss = 2.201\n",
      "Epoch  66 Batch   12/26   train_loss = 2.226\n",
      "Epoch  68 Batch   24/26   train_loss = 2.163\n",
      "Epoch  71 Batch   10/26   train_loss = 1.989\n",
      "Epoch  73 Batch   22/26   train_loss = 1.998\n",
      "Epoch  76 Batch    8/26   train_loss = 2.364\n",
      "Epoch  78 Batch   20/26   train_loss = 1.797\n",
      "Epoch  81 Batch    6/26   train_loss = 2.159\n",
      "Epoch  83 Batch   18/26   train_loss = 2.190\n",
      "Epoch  86 Batch    4/26   train_loss = 1.758\n",
      "Epoch  88 Batch   16/26   train_loss = 1.986\n",
      "Epoch  91 Batch    2/26   train_loss = 1.658\n",
      "Epoch  93 Batch   14/26   train_loss = 1.603\n",
      "Epoch  96 Batch    0/26   train_loss = 1.432\n",
      "Epoch  98 Batch   12/26   train_loss = 1.521\n",
      "Epoch 100 Batch   24/26   train_loss = 1.504\n",
      "Epoch 103 Batch   10/26   train_loss = 1.311\n",
      "Epoch 105 Batch   22/26   train_loss = 1.378\n",
      "Epoch 108 Batch    8/26   train_loss = 1.757\n",
      "Epoch 110 Batch   20/26   train_loss = 1.205\n",
      "Epoch 113 Batch    6/26   train_loss = 1.597\n",
      "Epoch 115 Batch   18/26   train_loss = 1.611\n",
      "Epoch 118 Batch    4/26   train_loss = 1.257\n",
      "Epoch 120 Batch   16/26   train_loss = 1.508\n",
      "Epoch 123 Batch    2/26   train_loss = 1.176\n",
      "Epoch 125 Batch   14/26   train_loss = 1.154\n",
      "Epoch 128 Batch    0/26   train_loss = 1.060\n",
      "Epoch 130 Batch   12/26   train_loss = 1.080\n",
      "Epoch 132 Batch   24/26   train_loss = 1.102\n",
      "Epoch 135 Batch   10/26   train_loss = 0.955\n",
      "Epoch 137 Batch   22/26   train_loss = 1.022\n",
      "Epoch 140 Batch    8/26   train_loss = 1.349\n",
      "Epoch 142 Batch   20/26   train_loss = 0.925\n",
      "Epoch 145 Batch    6/26   train_loss = 1.240\n",
      "Epoch 147 Batch   18/26   train_loss = 1.274\n",
      "Epoch 150 Batch    4/26   train_loss = 0.991\n",
      "Epoch 152 Batch   16/26   train_loss = 1.178\n",
      "Epoch 155 Batch    2/26   train_loss = 0.911\n",
      "Epoch 157 Batch   14/26   train_loss = 0.907\n",
      "Epoch 160 Batch    0/26   train_loss = 0.779\n",
      "Epoch 162 Batch   12/26   train_loss = 0.866\n",
      "Epoch 164 Batch   24/26   train_loss = 0.876\n",
      "Epoch 167 Batch   10/26   train_loss = 0.747\n",
      "Epoch 169 Batch   22/26   train_loss = 0.809\n",
      "Epoch 172 Batch    8/26   train_loss = 1.040\n",
      "Epoch 174 Batch   20/26   train_loss = 0.702\n",
      "Epoch 177 Batch    6/26   train_loss = 0.985\n",
      "Epoch 179 Batch   18/26   train_loss = 1.008\n",
      "Epoch 182 Batch    4/26   train_loss = 0.734\n",
      "Epoch 184 Batch   16/26   train_loss = 0.905\n",
      "Epoch 187 Batch    2/26   train_loss = 0.738\n",
      "Epoch 189 Batch   14/26   train_loss = 0.744\n",
      "Epoch 192 Batch    0/26   train_loss = 0.642\n",
      "Epoch 194 Batch   12/26   train_loss = 0.677\n",
      "Epoch 196 Batch   24/26   train_loss = 0.712\n",
      "Epoch 199 Batch   10/26   train_loss = 0.619\n",
      "Epoch 201 Batch   22/26   train_loss = 0.678\n",
      "Epoch 204 Batch    8/26   train_loss = 0.893\n",
      "Epoch 206 Batch   20/26   train_loss = 0.601\n",
      "Epoch 209 Batch    6/26   train_loss = 0.836\n",
      "Epoch 211 Batch   18/26   train_loss = 0.835\n",
      "Epoch 214 Batch    4/26   train_loss = 0.680\n",
      "Epoch 216 Batch   16/26   train_loss = 0.794\n",
      "Epoch 219 Batch    2/26   train_loss = 0.620\n",
      "Epoch 221 Batch   14/26   train_loss = 0.627\n",
      "Epoch 224 Batch    0/26   train_loss = 0.556\n",
      "Epoch 226 Batch   12/26   train_loss = 0.586\n",
      "Epoch 228 Batch   24/26   train_loss = 0.629\n",
      "Epoch 231 Batch   10/26   train_loss = 0.516\n",
      "Epoch 233 Batch   22/26   train_loss = 0.588\n",
      "Epoch 236 Batch    8/26   train_loss = 0.721\n",
      "Epoch 238 Batch   20/26   train_loss = 0.501\n",
      "Epoch 241 Batch    6/26   train_loss = 0.683\n",
      "Epoch 243 Batch   18/26   train_loss = 0.711\n",
      "Epoch 246 Batch    4/26   train_loss = 0.569\n",
      "Epoch 248 Batch   16/26   train_loss = 0.671\n",
      "Epoch 251 Batch    2/26   train_loss = 0.539\n",
      "Epoch 253 Batch   14/26   train_loss = 0.570\n",
      "Epoch 256 Batch    0/26   train_loss = 0.479\n",
      "Epoch 258 Batch   12/26   train_loss = 0.519\n",
      "Epoch 260 Batch   24/26   train_loss = 0.550\n",
      "Epoch 263 Batch   10/26   train_loss = 0.468\n",
      "Epoch 265 Batch   22/26   train_loss = 0.506\n",
      "Epoch 268 Batch    8/26   train_loss = 0.616\n",
      "Epoch 270 Batch   20/26   train_loss = 0.469\n",
      "Epoch 273 Batch    6/26   train_loss = 0.606\n",
      "Epoch 275 Batch   18/26   train_loss = 0.627\n",
      "Epoch 278 Batch    4/26   train_loss = 0.512\n",
      "Epoch 280 Batch   16/26   train_loss = 0.577\n",
      "Epoch 283 Batch    2/26   train_loss = 0.501\n",
      "Epoch 285 Batch   14/26   train_loss = 0.516\n",
      "Epoch 288 Batch    0/26   train_loss = 0.433\n",
      "Epoch 290 Batch   12/26   train_loss = 0.469\n",
      "Epoch 292 Batch   24/26   train_loss = 0.502\n",
      "Epoch 295 Batch   10/26   train_loss = 0.417\n",
      "Epoch 297 Batch   22/26   train_loss = 0.485\n",
      "Epoch 300 Batch    8/26   train_loss = 0.575\n",
      "Epoch 302 Batch   20/26   train_loss = 0.416\n",
      "Epoch 305 Batch    6/26   train_loss = 0.527\n",
      "Epoch 307 Batch   18/26   train_loss = 0.567\n",
      "Epoch 310 Batch    4/26   train_loss = 0.475\n",
      "Epoch 312 Batch   16/26   train_loss = 0.526\n",
      "Epoch 315 Batch    2/26   train_loss = 0.443\n",
      "Epoch 317 Batch   14/26   train_loss = 0.455\n",
      "Epoch 320 Batch    0/26   train_loss = 0.392\n",
      "Epoch 322 Batch   12/26   train_loss = 0.443\n",
      "Epoch 324 Batch   24/26   train_loss = 0.452\n",
      "Epoch 327 Batch   10/26   train_loss = 0.385\n",
      "Epoch 329 Batch   22/26   train_loss = 0.441\n",
      "Epoch 332 Batch    8/26   train_loss = 0.537\n",
      "Epoch 334 Batch   20/26   train_loss = 0.383\n",
      "Epoch 337 Batch    6/26   train_loss = 0.521\n",
      "Epoch 339 Batch   18/26   train_loss = 0.531\n",
      "Epoch 342 Batch    4/26   train_loss = 0.435\n",
      "Epoch 344 Batch   16/26   train_loss = 0.486\n",
      "Epoch 347 Batch    2/26   train_loss = 0.419\n",
      "Epoch 349 Batch   14/26   train_loss = 0.435\n",
      "Epoch 352 Batch    0/26   train_loss = 0.362\n",
      "Epoch 354 Batch   12/26   train_loss = 0.415\n",
      "Epoch 356 Batch   24/26   train_loss = 0.439\n",
      "Epoch 359 Batch   10/26   train_loss = 0.362\n",
      "Epoch 361 Batch   22/26   train_loss = 0.391\n",
      "Epoch 364 Batch    8/26   train_loss = 0.495\n",
      "Epoch 366 Batch   20/26   train_loss = 0.363\n",
      "Epoch 369 Batch    6/26   train_loss = 0.468\n",
      "Epoch 371 Batch   18/26   train_loss = 0.473\n",
      "Epoch 374 Batch    4/26   train_loss = 0.411\n",
      "Epoch 376 Batch   16/26   train_loss = 0.461\n",
      "Epoch 379 Batch    2/26   train_loss = 0.396\n",
      "Epoch 381 Batch   14/26   train_loss = 0.395\n",
      "Epoch 384 Batch    0/26   train_loss = 0.348\n",
      "Epoch 386 Batch   12/26   train_loss = 0.381\n",
      "Epoch 388 Batch   24/26   train_loss = 0.419\n",
      "Epoch 391 Batch   10/26   train_loss = 0.355\n",
      "Epoch 393 Batch   22/26   train_loss = 0.394\n",
      "Epoch 396 Batch    8/26   train_loss = 0.486\n",
      "Epoch 398 Batch   20/26   train_loss = 0.354\n",
      "Epoch 401 Batch    6/26   train_loss = 0.462\n",
      "Epoch 403 Batch   18/26   train_loss = 0.480\n",
      "Epoch 406 Batch    4/26   train_loss = 0.393\n",
      "Epoch 408 Batch   16/26   train_loss = 0.445\n",
      "Epoch 411 Batch    2/26   train_loss = 0.377\n",
      "Epoch 413 Batch   14/26   train_loss = 0.388\n",
      "Epoch 416 Batch    0/26   train_loss = 0.343\n",
      "Epoch 418 Batch   12/26   train_loss = 0.395\n",
      "Epoch 420 Batch   24/26   train_loss = 0.398\n",
      "Epoch 423 Batch   10/26   train_loss = 0.313\n",
      "Epoch 425 Batch   22/26   train_loss = 0.369\n",
      "Epoch 428 Batch    8/26   train_loss = 0.438\n",
      "Epoch 430 Batch   20/26   train_loss = 0.336\n",
      "Epoch 433 Batch    6/26   train_loss = 0.445\n",
      "Epoch 435 Batch   18/26   train_loss = 0.454\n",
      "Epoch 438 Batch    4/26   train_loss = 0.380\n",
      "Epoch 440 Batch   16/26   train_loss = 0.411\n",
      "Epoch 443 Batch    2/26   train_loss = 0.365\n",
      "Epoch 445 Batch   14/26   train_loss = 0.372\n",
      "Epoch 448 Batch    0/26   train_loss = 0.325\n",
      "Epoch 450 Batch   12/26   train_loss = 0.371\n",
      "Epoch 452 Batch   24/26   train_loss = 0.381\n",
      "Epoch 455 Batch   10/26   train_loss = 0.315\n",
      "Epoch 457 Batch   22/26   train_loss = 0.368\n",
      "Epoch 460 Batch    8/26   train_loss = 0.420\n",
      "Epoch 462 Batch   20/26   train_loss = 0.322\n",
      "Epoch 465 Batch    6/26   train_loss = 0.424\n",
      "Epoch 467 Batch   18/26   train_loss = 0.419\n",
      "Epoch 470 Batch    4/26   train_loss = 0.384\n",
      "Epoch 472 Batch   16/26   train_loss = 0.415\n",
      "Epoch 475 Batch    2/26   train_loss = 0.343\n",
      "Epoch 477 Batch   14/26   train_loss = 0.358\n",
      "Epoch 480 Batch    0/26   train_loss = 0.314\n",
      "Epoch 482 Batch   12/26   train_loss = 0.349\n",
      "Epoch 484 Batch   24/26   train_loss = 0.358\n",
      "Epoch 487 Batch   10/26   train_loss = 0.322\n",
      "Epoch 489 Batch   22/26   train_loss = 0.343\n",
      "Epoch 492 Batch    8/26   train_loss = 0.422\n",
      "Epoch 494 Batch   20/26   train_loss = 0.318\n",
      "Epoch 497 Batch    6/26   train_loss = 0.399\n",
      "Epoch 499 Batch   18/26   train_loss = 0.422\n",
      "Epoch 502 Batch    4/26   train_loss = 0.365\n",
      "Epoch 504 Batch   16/26   train_loss = 0.384\n",
      "Epoch 507 Batch    2/26   train_loss = 0.349\n",
      "Epoch 509 Batch   14/26   train_loss = 0.349\n",
      "Epoch 512 Batch    0/26   train_loss = 0.306\n",
      "Epoch 514 Batch   12/26   train_loss = 0.341\n",
      "Epoch 516 Batch   24/26   train_loss = 0.347\n",
      "Epoch 519 Batch   10/26   train_loss = 0.292\n",
      "Epoch 521 Batch   22/26   train_loss = 0.331\n",
      "Epoch 524 Batch    8/26   train_loss = 0.406\n",
      "Epoch 526 Batch   20/26   train_loss = 0.321\n",
      "Epoch 529 Batch    6/26   train_loss = 0.387\n",
      "Epoch 531 Batch   18/26   train_loss = 0.389\n",
      "Epoch 534 Batch    4/26   train_loss = 0.353\n",
      "Epoch 536 Batch   16/26   train_loss = 0.380\n",
      "Epoch 539 Batch    2/26   train_loss = 0.338\n",
      "Epoch 541 Batch   14/26   train_loss = 0.333\n",
      "Epoch 544 Batch    0/26   train_loss = 0.309\n",
      "Epoch 546 Batch   12/26   train_loss = 0.333\n",
      "Epoch 548 Batch   24/26   train_loss = 0.352\n",
      "Epoch 551 Batch   10/26   train_loss = 0.304\n",
      "Epoch 553 Batch   22/26   train_loss = 0.339\n",
      "Epoch 556 Batch    8/26   train_loss = 0.399\n",
      "Epoch 558 Batch   20/26   train_loss = 0.299\n",
      "Epoch 561 Batch    6/26   train_loss = 0.391\n",
      "Epoch 563 Batch   18/26   train_loss = 0.394\n",
      "Epoch 566 Batch    4/26   train_loss = 0.335\n",
      "Epoch 568 Batch   16/26   train_loss = 0.376\n",
      "Epoch 571 Batch    2/26   train_loss = 0.321\n",
      "Epoch 573 Batch   14/26   train_loss = 0.338\n",
      "Epoch 576 Batch    0/26   train_loss = 0.299\n",
      "Epoch 578 Batch   12/26   train_loss = 0.343\n",
      "Epoch 580 Batch   24/26   train_loss = 0.333\n",
      "Epoch 583 Batch   10/26   train_loss = 0.297\n",
      "Epoch 585 Batch   22/26   train_loss = 0.333\n",
      "Epoch 588 Batch    8/26   train_loss = 0.385\n",
      "Epoch 590 Batch   20/26   train_loss = 0.293\n",
      "Epoch 593 Batch    6/26   train_loss = 0.382\n",
      "Epoch 595 Batch   18/26   train_loss = 0.377\n",
      "Epoch 598 Batch    4/26   train_loss = 0.334\n",
      "Epoch 600 Batch   16/26   train_loss = 0.373\n",
      "Epoch 603 Batch    2/26   train_loss = 0.325\n",
      "Epoch 605 Batch   14/26   train_loss = 0.319\n",
      "Epoch 608 Batch    0/26   train_loss = 0.290\n",
      "Epoch 610 Batch   12/26   train_loss = 0.322\n",
      "Epoch 612 Batch   24/26   train_loss = 0.332\n",
      "Epoch 615 Batch   10/26   train_loss = 0.297\n",
      "Epoch 617 Batch   22/26   train_loss = 0.325\n",
      "Epoch 620 Batch    8/26   train_loss = 0.370\n",
      "Epoch 622 Batch   20/26   train_loss = 0.286\n",
      "Epoch 625 Batch    6/26   train_loss = 0.362\n",
      "Epoch 627 Batch   18/26   train_loss = 0.371\n",
      "Epoch 630 Batch    4/26   train_loss = 0.333\n",
      "Epoch 632 Batch   16/26   train_loss = 0.351\n",
      "Epoch 635 Batch    2/26   train_loss = 0.322\n",
      "Epoch 637 Batch   14/26   train_loss = 0.321\n",
      "Epoch 640 Batch    0/26   train_loss = 0.283\n",
      "Epoch 642 Batch   12/26   train_loss = 0.319\n",
      "Epoch 644 Batch   24/26   train_loss = 0.325\n",
      "Epoch 647 Batch   10/26   train_loss = 0.287\n",
      "Epoch 649 Batch   22/26   train_loss = 0.313\n",
      "Epoch 652 Batch    8/26   train_loss = 0.374\n",
      "Epoch 654 Batch   20/26   train_loss = 0.293\n",
      "Epoch 657 Batch    6/26   train_loss = 0.350\n",
      "Epoch 659 Batch   18/26   train_loss = 0.372\n",
      "Epoch 662 Batch    4/26   train_loss = 0.319\n",
      "Epoch 664 Batch   16/26   train_loss = 0.364\n",
      "Epoch 667 Batch    2/26   train_loss = 0.318\n",
      "Epoch 669 Batch   14/26   train_loss = 0.317\n",
      "Epoch 672 Batch    0/26   train_loss = 0.279\n",
      "Epoch 674 Batch   12/26   train_loss = 0.303\n",
      "Epoch 676 Batch   24/26   train_loss = 0.315\n",
      "Epoch 679 Batch   10/26   train_loss = 0.278\n",
      "Epoch 681 Batch   22/26   train_loss = 0.314\n",
      "Epoch 684 Batch    8/26   train_loss = 0.368\n",
      "Epoch 686 Batch   20/26   train_loss = 0.291\n",
      "Epoch 689 Batch    6/26   train_loss = 0.365\n",
      "Epoch 691 Batch   18/26   train_loss = 0.365\n",
      "Epoch 694 Batch    4/26   train_loss = 0.306\n",
      "Epoch 696 Batch   16/26   train_loss = 0.356\n",
      "Epoch 699 Batch    2/26   train_loss = 0.306\n",
      "Epoch 701 Batch   14/26   train_loss = 0.316\n",
      "Epoch 704 Batch    0/26   train_loss = 0.279\n",
      "Epoch 706 Batch   12/26   train_loss = 0.329\n",
      "Epoch 708 Batch   24/26   train_loss = 0.315\n",
      "Epoch 711 Batch   10/26   train_loss = 0.279\n",
      "Epoch 713 Batch   22/26   train_loss = 0.312\n",
      "Epoch 716 Batch    8/26   train_loss = 0.366\n",
      "Epoch 718 Batch   20/26   train_loss = 0.275\n",
      "Epoch 721 Batch    6/26   train_loss = 0.348\n",
      "Epoch 723 Batch   18/26   train_loss = 0.361\n",
      "Epoch 726 Batch    4/26   train_loss = 0.310\n",
      "Epoch 728 Batch   16/26   train_loss = 0.348\n",
      "Epoch 731 Batch    2/26   train_loss = 0.316\n",
      "Epoch 733 Batch   14/26   train_loss = 0.317\n",
      "Epoch 736 Batch    0/26   train_loss = 0.275\n",
      "Epoch 738 Batch   12/26   train_loss = 0.310\n",
      "Epoch 740 Batch   24/26   train_loss = 0.309\n",
      "Epoch 743 Batch   10/26   train_loss = 0.277\n",
      "Epoch 745 Batch   22/26   train_loss = 0.312\n",
      "Epoch 748 Batch    8/26   train_loss = 0.354\n",
      "Epoch 750 Batch   20/26   train_loss = 0.292\n",
      "Epoch 753 Batch    6/26   train_loss = 0.340\n",
      "Epoch 755 Batch   18/26   train_loss = 0.357\n",
      "Epoch 758 Batch    4/26   train_loss = 0.311\n",
      "Epoch 760 Batch   16/26   train_loss = 0.344\n",
      "Epoch 763 Batch    2/26   train_loss = 0.310\n",
      "Epoch 765 Batch   14/26   train_loss = 0.305\n",
      "Epoch 768 Batch    0/26   train_loss = 0.277\n",
      "Epoch 770 Batch   12/26   train_loss = 0.309\n",
      "Epoch 772 Batch   24/26   train_loss = 0.316\n",
      "Epoch 775 Batch   10/26   train_loss = 0.270\n",
      "Epoch 777 Batch   22/26   train_loss = 0.294\n",
      "Epoch 780 Batch    8/26   train_loss = 0.353\n",
      "Epoch 782 Batch   20/26   train_loss = 0.286\n",
      "Epoch 785 Batch    6/26   train_loss = 0.350\n",
      "Epoch 787 Batch   18/26   train_loss = 0.359\n",
      "Epoch 790 Batch    4/26   train_loss = 0.310\n",
      "Epoch 792 Batch   16/26   train_loss = 0.334\n",
      "Epoch 795 Batch    2/26   train_loss = 0.303\n",
      "Epoch 797 Batch   14/26   train_loss = 0.305\n",
      "Epoch 800 Batch    0/26   train_loss = 0.264\n",
      "Epoch 802 Batch   12/26   train_loss = 0.312\n",
      "Epoch 804 Batch   24/26   train_loss = 0.305\n",
      "Epoch 807 Batch   10/26   train_loss = 0.254\n",
      "Epoch 809 Batch   22/26   train_loss = 0.306\n",
      "Epoch 812 Batch    8/26   train_loss = 0.350\n",
      "Epoch 814 Batch   20/26   train_loss = 0.279\n",
      "Epoch 817 Batch    6/26   train_loss = 0.340\n",
      "Epoch 819 Batch   18/26   train_loss = 0.339\n",
      "Epoch 822 Batch    4/26   train_loss = 0.311\n",
      "Epoch 824 Batch   16/26   train_loss = 0.333\n",
      "Epoch 827 Batch    2/26   train_loss = 0.312\n",
      "Epoch 829 Batch   14/26   train_loss = 0.315\n",
      "Epoch 832 Batch    0/26   train_loss = 0.272\n",
      "Epoch 834 Batch   12/26   train_loss = 0.301\n",
      "Epoch 836 Batch   24/26   train_loss = 0.304\n",
      "Epoch 839 Batch   10/26   train_loss = 0.260\n",
      "Epoch 841 Batch   22/26   train_loss = 0.306\n",
      "Epoch 844 Batch    8/26   train_loss = 0.353\n",
      "Epoch 846 Batch   20/26   train_loss = 0.277\n",
      "Epoch 849 Batch    6/26   train_loss = 0.333\n",
      "Epoch 851 Batch   18/26   train_loss = 0.341\n",
      "Epoch 854 Batch    4/26   train_loss = 0.304\n",
      "Epoch 856 Batch   16/26   train_loss = 0.331\n",
      "Epoch 859 Batch    2/26   train_loss = 0.305\n",
      "Epoch 861 Batch   14/26   train_loss = 0.284\n",
      "Epoch 864 Batch    0/26   train_loss = 0.266\n",
      "Epoch 866 Batch   12/26   train_loss = 0.298\n",
      "Epoch 868 Batch   24/26   train_loss = 0.295\n",
      "Epoch 871 Batch   10/26   train_loss = 0.267\n",
      "Epoch 873 Batch   22/26   train_loss = 0.296\n",
      "Epoch 876 Batch    8/26   train_loss = 0.347\n",
      "Epoch 878 Batch   20/26   train_loss = 0.272\n",
      "Epoch 881 Batch    6/26   train_loss = 0.327\n",
      "Epoch 883 Batch   18/26   train_loss = 0.352\n",
      "Epoch 886 Batch    4/26   train_loss = 0.301\n",
      "Epoch 888 Batch   16/26   train_loss = 0.326\n",
      "Epoch 891 Batch    2/26   train_loss = 0.298\n",
      "Epoch 893 Batch   14/26   train_loss = 0.296\n",
      "Epoch 896 Batch    0/26   train_loss = 0.266\n",
      "Epoch 898 Batch   12/26   train_loss = 0.293\n",
      "Epoch 900 Batch   24/26   train_loss = 0.292\n",
      "Epoch 903 Batch   10/26   train_loss = 0.254\n",
      "Epoch 905 Batch   22/26   train_loss = 0.297\n",
      "Epoch 908 Batch    8/26   train_loss = 0.339\n",
      "Epoch 910 Batch   20/26   train_loss = 0.266\n",
      "Epoch 913 Batch    6/26   train_loss = 0.325\n",
      "Epoch 915 Batch   18/26   train_loss = 0.345\n",
      "Epoch 918 Batch    4/26   train_loss = 0.305\n",
      "Epoch 920 Batch   16/26   train_loss = 0.327\n",
      "Epoch 923 Batch    2/26   train_loss = 0.290\n",
      "Epoch 925 Batch   14/26   train_loss = 0.300\n",
      "Epoch 928 Batch    0/26   train_loss = 0.259\n",
      "Epoch 930 Batch   12/26   train_loss = 0.299\n",
      "Epoch 932 Batch   24/26   train_loss = 0.295\n",
      "Epoch 935 Batch   10/26   train_loss = 0.269\n",
      "Epoch 937 Batch   22/26   train_loss = 0.295\n",
      "Epoch 940 Batch    8/26   train_loss = 0.345\n",
      "Epoch 942 Batch   20/26   train_loss = 0.270\n",
      "Epoch 945 Batch    6/26   train_loss = 0.336\n",
      "Epoch 947 Batch   18/26   train_loss = 0.339\n",
      "Epoch 950 Batch    4/26   train_loss = 0.295\n",
      "Epoch 952 Batch   16/26   train_loss = 0.316\n",
      "Epoch 955 Batch    2/26   train_loss = 0.304\n",
      "Epoch 957 Batch   14/26   train_loss = 0.287\n",
      "Epoch 960 Batch    0/26   train_loss = 0.266\n",
      "Epoch 962 Batch   12/26   train_loss = 0.296\n",
      "Epoch 964 Batch   24/26   train_loss = 0.299\n",
      "Epoch 967 Batch   10/26   train_loss = 0.262\n",
      "Epoch 969 Batch   22/26   train_loss = 0.293\n",
      "Epoch 972 Batch    8/26   train_loss = 0.336\n",
      "Epoch 974 Batch   20/26   train_loss = 0.250\n",
      "Epoch 977 Batch    6/26   train_loss = 0.327\n",
      "Epoch 979 Batch   18/26   train_loss = 0.343\n",
      "Epoch 982 Batch    4/26   train_loss = 0.295\n",
      "Epoch 984 Batch   16/26   train_loss = 0.319\n",
      "Epoch 987 Batch    2/26   train_loss = 0.286\n",
      "Epoch 989 Batch   14/26   train_loss = 0.292\n",
      "Epoch 992 Batch    0/26   train_loss = 0.256\n",
      "Epoch 994 Batch   12/26   train_loss = 0.287\n",
      "Epoch 996 Batch   24/26   train_loss = 0.292\n",
      "Epoch 999 Batch   10/26   train_loss = 0.270\n",
      "Epoch 1001 Batch   22/26   train_loss = 0.298\n",
      "Epoch 1004 Batch    8/26   train_loss = 0.335\n",
      "Epoch 1006 Batch   20/26   train_loss = 0.268\n",
      "Epoch 1009 Batch    6/26   train_loss = 0.319\n",
      "Epoch 1011 Batch   18/26   train_loss = 0.327\n",
      "Epoch 1014 Batch    4/26   train_loss = 0.294\n",
      "Epoch 1016 Batch   16/26   train_loss = 0.320\n",
      "Epoch 1019 Batch    2/26   train_loss = 0.294\n",
      "Epoch 1021 Batch   14/26   train_loss = 0.283\n",
      "Epoch 1024 Batch    0/26   train_loss = 0.252\n",
      "Epoch 1026 Batch   12/26   train_loss = 0.294\n",
      "Epoch 1028 Batch   24/26   train_loss = 0.283\n",
      "Epoch 1031 Batch   10/26   train_loss = 0.253\n",
      "Epoch 1033 Batch   22/26   train_loss = 0.294\n",
      "Epoch 1036 Batch    8/26   train_loss = 0.326\n",
      "Epoch 1038 Batch   20/26   train_loss = 0.260\n",
      "Epoch 1041 Batch    6/26   train_loss = 0.320\n",
      "Epoch 1043 Batch   18/26   train_loss = 0.311\n",
      "Epoch 1046 Batch    4/26   train_loss = 0.286\n",
      "Epoch 1048 Batch   16/26   train_loss = 0.330\n",
      "Epoch 1051 Batch    2/26   train_loss = 0.283\n",
      "Epoch 1053 Batch   14/26   train_loss = 0.288\n",
      "Epoch 1056 Batch    0/26   train_loss = 0.255\n",
      "Epoch 1058 Batch   12/26   train_loss = 0.288\n",
      "Epoch 1060 Batch   24/26   train_loss = 0.296\n",
      "Epoch 1063 Batch   10/26   train_loss = 0.250\n",
      "Epoch 1065 Batch   22/26   train_loss = 0.281\n",
      "Epoch 1068 Batch    8/26   train_loss = 0.326\n",
      "Epoch 1070 Batch   20/26   train_loss = 0.265\n",
      "Epoch 1073 Batch    6/26   train_loss = 0.318\n",
      "Epoch 1075 Batch   18/26   train_loss = 0.321\n",
      "Epoch 1078 Batch    4/26   train_loss = 0.283\n",
      "Epoch 1080 Batch   16/26   train_loss = 0.323\n",
      "Epoch 1083 Batch    2/26   train_loss = 0.282\n",
      "Epoch 1085 Batch   14/26   train_loss = 0.290\n",
      "Epoch 1088 Batch    0/26   train_loss = 0.258\n",
      "Epoch 1090 Batch   12/26   train_loss = 0.292\n",
      "Epoch 1092 Batch   24/26   train_loss = 0.290\n",
      "Epoch 1095 Batch   10/26   train_loss = 0.253\n",
      "Epoch 1097 Batch   22/26   train_loss = 0.288\n",
      "Epoch 1100 Batch    8/26   train_loss = 0.318\n",
      "Epoch 1102 Batch   20/26   train_loss = 0.263\n",
      "Epoch 1105 Batch    6/26   train_loss = 0.325\n",
      "Epoch 1107 Batch   18/26   train_loss = 0.323\n",
      "Epoch 1110 Batch    4/26   train_loss = 0.288\n",
      "Epoch 1112 Batch   16/26   train_loss = 0.313\n",
      "Epoch 1115 Batch    2/26   train_loss = 0.283\n",
      "Epoch 1117 Batch   14/26   train_loss = 0.277\n",
      "Epoch 1120 Batch    0/26   train_loss = 0.247\n",
      "Epoch 1122 Batch   12/26   train_loss = 0.282\n",
      "Epoch 1124 Batch   24/26   train_loss = 0.290\n",
      "Epoch 1127 Batch   10/26   train_loss = 0.256\n",
      "Epoch 1129 Batch   22/26   train_loss = 0.292\n",
      "Epoch 1132 Batch    8/26   train_loss = 0.325\n",
      "Epoch 1134 Batch   20/26   train_loss = 0.249\n",
      "Epoch 1137 Batch    6/26   train_loss = 0.316\n",
      "Epoch 1139 Batch   18/26   train_loss = 0.314\n",
      "Epoch 1142 Batch    4/26   train_loss = 0.292\n",
      "Epoch 1144 Batch   16/26   train_loss = 0.308\n",
      "Epoch 1147 Batch    2/26   train_loss = 0.281\n",
      "Epoch 1149 Batch   14/26   train_loss = 0.272\n",
      "Epoch 1152 Batch    0/26   train_loss = 0.256\n",
      "Epoch 1154 Batch   12/26   train_loss = 0.283\n",
      "Epoch 1156 Batch   24/26   train_loss = 0.294\n",
      "Epoch 1159 Batch   10/26   train_loss = 0.254\n",
      "Epoch 1161 Batch   22/26   train_loss = 0.287\n",
      "Epoch 1164 Batch    8/26   train_loss = 0.319\n",
      "Epoch 1166 Batch   20/26   train_loss = 0.255\n",
      "Epoch 1169 Batch    6/26   train_loss = 0.318\n",
      "Epoch 1171 Batch   18/26   train_loss = 0.319\n",
      "Epoch 1174 Batch    4/26   train_loss = 0.281\n",
      "Epoch 1176 Batch   16/26   train_loss = 0.319\n",
      "Epoch 1179 Batch    2/26   train_loss = 0.277\n",
      "Epoch 1181 Batch   14/26   train_loss = 0.290\n",
      "Epoch 1184 Batch    0/26   train_loss = 0.252\n",
      "Epoch 1186 Batch   12/26   train_loss = 0.287\n",
      "Epoch 1188 Batch   24/26   train_loss = 0.285\n",
      "Epoch 1191 Batch   10/26   train_loss = 0.250\n",
      "Epoch 1193 Batch   22/26   train_loss = 0.283\n",
      "Epoch 1196 Batch    8/26   train_loss = 0.323\n",
      "Epoch 1198 Batch   20/26   train_loss = 0.255\n",
      "Epoch 1201 Batch    6/26   train_loss = 0.322\n",
      "Epoch 1203 Batch   18/26   train_loss = 0.320\n",
      "Epoch 1206 Batch    4/26   train_loss = 0.291\n",
      "Epoch 1208 Batch   16/26   train_loss = 0.303\n",
      "Epoch 1211 Batch    2/26   train_loss = 0.277\n",
      "Epoch 1213 Batch   14/26   train_loss = 0.283\n",
      "Epoch 1216 Batch    0/26   train_loss = 0.257\n",
      "Epoch 1218 Batch   12/26   train_loss = 0.283\n",
      "Epoch 1220 Batch   24/26   train_loss = 0.280\n",
      "Epoch 1223 Batch   10/26   train_loss = 0.252\n",
      "Epoch 1225 Batch   22/26   train_loss = 0.286\n",
      "Epoch 1228 Batch    8/26   train_loss = 0.323\n",
      "Epoch 1230 Batch   20/26   train_loss = 0.258\n",
      "Epoch 1233 Batch    6/26   train_loss = 0.325\n",
      "Epoch 1235 Batch   18/26   train_loss = 0.314\n",
      "Epoch 1238 Batch    4/26   train_loss = 0.294\n",
      "Epoch 1240 Batch   16/26   train_loss = 0.304\n",
      "Epoch 1243 Batch    2/26   train_loss = 0.278\n",
      "Epoch 1245 Batch   14/26   train_loss = 0.283\n",
      "Epoch 1248 Batch    0/26   train_loss = 0.253\n",
      "Epoch 1250 Batch   12/26   train_loss = 0.280\n",
      "Epoch 1252 Batch   24/26   train_loss = 0.282\n",
      "Epoch 1255 Batch   10/26   train_loss = 0.256\n",
      "Epoch 1257 Batch   22/26   train_loss = 0.281\n",
      "Epoch 1260 Batch    8/26   train_loss = 0.328\n",
      "Epoch 1262 Batch   20/26   train_loss = 0.261\n",
      "Epoch 1265 Batch    6/26   train_loss = 0.315\n",
      "Epoch 1267 Batch   18/26   train_loss = 0.324\n",
      "Epoch 1270 Batch    4/26   train_loss = 0.287\n",
      "Epoch 1272 Batch   16/26   train_loss = 0.309\n",
      "Epoch 1275 Batch    2/26   train_loss = 0.275\n",
      "Epoch 1277 Batch   14/26   train_loss = 0.282\n",
      "Epoch 1280 Batch    0/26   train_loss = 0.245\n",
      "Epoch 1282 Batch   12/26   train_loss = 0.278\n",
      "Epoch 1284 Batch   24/26   train_loss = 0.289\n",
      "Epoch 1287 Batch   10/26   train_loss = 0.242\n",
      "Epoch 1289 Batch   22/26   train_loss = 0.277\n",
      "Epoch 1292 Batch    8/26   train_loss = 0.318\n",
      "Epoch 1294 Batch   20/26   train_loss = 0.252\n",
      "Epoch 1297 Batch    6/26   train_loss = 0.322\n",
      "Epoch 1299 Batch   18/26   train_loss = 0.314\n",
      "Epoch 1302 Batch    4/26   train_loss = 0.279\n",
      "Epoch 1304 Batch   16/26   train_loss = 0.308\n",
      "Epoch 1307 Batch    2/26   train_loss = 0.281\n",
      "Epoch 1309 Batch   14/26   train_loss = 0.282\n",
      "Epoch 1312 Batch    0/26   train_loss = 0.245\n",
      "Epoch 1314 Batch   12/26   train_loss = 0.277\n",
      "Epoch 1316 Batch   24/26   train_loss = 0.289\n",
      "Epoch 1319 Batch   10/26   train_loss = 0.245\n",
      "Epoch 1321 Batch   22/26   train_loss = 0.282\n",
      "Epoch 1324 Batch    8/26   train_loss = 0.317\n",
      "Epoch 1326 Batch   20/26   train_loss = 0.252\n",
      "Epoch 1329 Batch    6/26   train_loss = 0.311\n",
      "Epoch 1331 Batch   18/26   train_loss = 0.320\n",
      "Epoch 1334 Batch    4/26   train_loss = 0.283\n",
      "Epoch 1336 Batch   16/26   train_loss = 0.307\n",
      "Epoch 1339 Batch    2/26   train_loss = 0.279\n",
      "Epoch 1341 Batch   14/26   train_loss = 0.285\n",
      "Epoch 1344 Batch    0/26   train_loss = 0.249\n",
      "Epoch 1346 Batch   12/26   train_loss = 0.272\n",
      "Epoch 1348 Batch   24/26   train_loss = 0.280\n",
      "Epoch 1351 Batch   10/26   train_loss = 0.253\n",
      "Epoch 1353 Batch   22/26   train_loss = 0.280\n",
      "Epoch 1356 Batch    8/26   train_loss = 0.312\n",
      "Epoch 1358 Batch   20/26   train_loss = 0.248\n",
      "Epoch 1361 Batch    6/26   train_loss = 0.320\n",
      "Epoch 1363 Batch   18/26   train_loss = 0.324\n",
      "Epoch 1366 Batch    4/26   train_loss = 0.282\n",
      "Epoch 1368 Batch   16/26   train_loss = 0.308\n",
      "Epoch 1371 Batch    2/26   train_loss = 0.276\n",
      "Epoch 1373 Batch   14/26   train_loss = 0.273\n",
      "Epoch 1376 Batch    0/26   train_loss = 0.240\n",
      "Epoch 1378 Batch   12/26   train_loss = 0.281\n",
      "Epoch 1380 Batch   24/26   train_loss = 0.283\n",
      "Epoch 1383 Batch   10/26   train_loss = 0.251\n",
      "Epoch 1385 Batch   22/26   train_loss = 0.281\n",
      "Epoch 1388 Batch    8/26   train_loss = 0.317\n",
      "Epoch 1390 Batch   20/26   train_loss = 0.249\n",
      "Epoch 1393 Batch    6/26   train_loss = 0.311\n",
      "Epoch 1395 Batch   18/26   train_loss = 0.312\n",
      "Epoch 1398 Batch    4/26   train_loss = 0.287\n",
      "Epoch 1400 Batch   16/26   train_loss = 0.303\n",
      "Epoch 1403 Batch    2/26   train_loss = 0.276\n",
      "Epoch 1405 Batch   14/26   train_loss = 0.275\n",
      "Epoch 1408 Batch    0/26   train_loss = 0.244\n",
      "Epoch 1410 Batch   12/26   train_loss = 0.275\n",
      "Epoch 1412 Batch   24/26   train_loss = 0.270\n",
      "Epoch 1415 Batch   10/26   train_loss = 0.238\n",
      "Epoch 1417 Batch   22/26   train_loss = 0.283\n",
      "Epoch 1420 Batch    8/26   train_loss = 0.316\n",
      "Epoch 1422 Batch   20/26   train_loss = 0.248\n",
      "Epoch 1425 Batch    6/26   train_loss = 0.308\n",
      "Epoch 1427 Batch   18/26   train_loss = 0.322\n",
      "Epoch 1430 Batch    4/26   train_loss = 0.279\n",
      "Epoch 1432 Batch   16/26   train_loss = 0.302\n",
      "Epoch 1435 Batch    2/26   train_loss = 0.282\n",
      "Epoch 1437 Batch   14/26   train_loss = 0.273\n",
      "Epoch 1440 Batch    0/26   train_loss = 0.248\n",
      "Epoch 1442 Batch   12/26   train_loss = 0.281\n",
      "Epoch 1444 Batch   24/26   train_loss = 0.280\n",
      "Epoch 1447 Batch   10/26   train_loss = 0.243\n",
      "Epoch 1449 Batch   22/26   train_loss = 0.273\n",
      "Epoch 1452 Batch    8/26   train_loss = 0.312\n",
      "Epoch 1454 Batch   20/26   train_loss = 0.243\n",
      "Epoch 1457 Batch    6/26   train_loss = 0.302\n",
      "Epoch 1459 Batch   18/26   train_loss = 0.321\n",
      "Epoch 1462 Batch    4/26   train_loss = 0.274\n",
      "Epoch 1464 Batch   16/26   train_loss = 0.300\n",
      "Epoch 1467 Batch    2/26   train_loss = 0.278\n",
      "Epoch 1469 Batch   14/26   train_loss = 0.273\n",
      "Epoch 1472 Batch    0/26   train_loss = 0.244\n",
      "Epoch 1474 Batch   12/26   train_loss = 0.278\n",
      "Epoch 1476 Batch   24/26   train_loss = 0.275\n",
      "Epoch 1479 Batch   10/26   train_loss = 0.245\n",
      "Epoch 1481 Batch   22/26   train_loss = 0.278\n",
      "Epoch 1484 Batch    8/26   train_loss = 0.321\n",
      "Epoch 1486 Batch   20/26   train_loss = 0.247\n",
      "Epoch 1489 Batch    6/26   train_loss = 0.310\n",
      "Epoch 1491 Batch   18/26   train_loss = 0.306\n",
      "Epoch 1494 Batch    4/26   train_loss = 0.278\n",
      "Epoch 1496 Batch   16/26   train_loss = 0.300\n",
      "Epoch 1499 Batch    2/26   train_loss = 0.281\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save Parameters\n",
    "Save `seq_length` and `save_dir` for generating a new TV script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Generate Functions\n",
    "### Get Tensors\n",
    "Get tensors from `loaded_graph` using the function [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name).  Get the tensors using the following names:\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "Return the tensors in the following tuple `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_tensor = loaded_graph.get_tensor_by_name('input:0')\n",
    "    initial_state_tensor = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state_tensor = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs_tensor = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    \n",
    "    return input_tensor, initial_state_tensor, final_state_tensor, probs_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Choose Word\n",
    "Implement the `pick_word()` function to select the next word using `probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function   \n",
    "    # don't necessarily pick the best word, since this is a small dataset.\n",
    "    # pick one of the best words instead, consider the top 5\n",
    "    \n",
    "    # sort array, to get indices in descending order of probability\n",
    "#     sorted_indices = np.argsort(probabilities)\n",
    "#     index_of_next_word = np.random.choice(sorted_indices[:5])\n",
    "    return int_to_vocab[np.argmax(probabilities)]\n",
    "#     return int_to_vocab[index_of_next_word]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate TV Script\n",
    "This will generate the TV script for you.  Set `gen_length` to the length of TV script you want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homer_simpson:(flatly) whatever.\n",
      "marge_simpson: homie, what's.\n",
      "adult_bart: right to the texas cheesecake depository a beer.\n",
      "moe_szyslak:(pointed) okay..(gasp)\n",
      "homer_simpson:(grunt)(very phone) yeah...\n",
      "homer_simpson:(rolls of here, happily) i'm good there. i don't get all an.\n",
      "\n",
      "\n",
      "carl_carlson: hey, homer, and you come i've gotta moe!\n",
      "\n",
      "\n",
      "homer_simpson: don't did you start, and this happen to nine, homer...\n",
      "\n",
      "\n",
      "lenny_leonard: is this, and i wanna so you, or so a think of i'm gonna be...\n",
      "moe_szyslak: oh., don't kick no of you...\n",
      "moe_szyslak:(pulls out,) super...\n",
      "all: look, and i never did.\n",
      "lenny_leonard: oh, he was a homer in it!\n",
      "\n",
      "moe_szyslak: hey, homer., you what am i do.\n",
      "bart_simpson: what's the\".\" the phone) that's it is it. do i feel.\n",
      "patty_bouvier: is this?\n",
      "homer_simpson: you, some like twelve. just let me check with the hospital one.\n",
      "moe_szyslak: i can, mr.. that...\n",
      "moe_szyslak: okay, and the funniest ingredient...\n",
      "homer_simpson:(impressed) an unattended who are you guys. i was sorry you-- and is the right, here.\n",
      "carl_carlson: moe, why don't you get that out?!\n",
      "cletus_spuckler: for your fifty cents and i became not to you...\n",
      "homer_simpson:(weirded-out noises)) that's my little slugger.\n",
      "carl_carlson:(whistles) his dog ever)..\n",
      "moe_szyslak:(quietly here good) i are!\n",
      "carl_carlson: and, what are you and learn?\n",
      "narrator: who?\n",
      "\n",
      "\n",
      "homer_simpson:(gasps up here okay you! this good old pal. i haven't seen you!\n",
      "\n",
      "\n",
      "friend you, freak? that's gotta be another\n",
      "lisa_simpson: i'm a big day, and i just be gettin' you!\n",
      "moe_szyslak: hey, would long funny. uh i, hey.\n",
      "homer_simpson:(to self, homer!\n",
      "ned_flanders: yeah, looks like i have help! jeez.\n",
      "moe_szyslak: you!. your a that's been you. what did something he didn't have a\"\n",
      "ned_flanders:\" super carl. by.\n",
      "homer_simpson: it's one.\n",
      "moe_szyslak: ah, moe? i was a stupid who into a life, get?\n",
      "hans:(sings) sweet me!..\n",
      "moe_szyslak:(into phone) yeah.. unless.. gets?\n",
      "homer_simpson: lenny, carl.\n",
      "homer_simpson:(big smile,\" duh\") are you, don't gotta sell that has to meet the crap? i'll just come to you, this once was the problem. you don't think marge.\n",
      "homer_simpson: to look at you, moe. i was. and here's the rope!\n",
      "moe_szyslak: yeah, maybe burns? we can get a camera offa me and springfield, all right. i could really?\n",
      "homer_simpson:(to self, homer?\n",
      "barney_gumble: oh, you... your lost a love of my night through...\n",
      "moe_szyslak: you, moe, not. i'm gonna drink something.\n",
      "krusty_the_clown: look at the super. i'm lookin' in! / i'm not) i could never let me or anything. back.\n",
      "lenny_leonard: sounds like to if it only good... ah, i got to the new liver, and you can you still?\n",
      "homer_simpson: okay, and make me not a weird if you don't do.\n",
      "\n",
      "\n",
      "carl_carlson: yeah, how call it. we'd like something.\n",
      "lenny_leonard: hey, how ya, i used to pick around you. moe cure what do about a to see.\n",
      "moe_szyslak:(playful)! keep your friend.... that doesn't know what the best year of a\" flaming moe\n",
      "\n",
      "\n",
      "lenny_leonard:(camera) of where i get all you to the be ago.(laughs)\" old the\n",
      "lenny_leonard:(sotto,) my way to not the treasure?\n",
      "lenny_leonard: not too.\n",
      "lenny_leonard: you, homer.\n"
     ]
    }
   ],
   "source": [
    "gen_length = 800\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'homer_simpson'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# The TV Script is Nonsensical\n",
    "It's ok if the TV script doesn't make any sense.  We trained on less than a megabyte of text.  In order to get good results, you'll have to use a smaller vocabulary or get more data.  Luckly there's more data!  As we mentioned in the begging of this project, this is a subset of [another dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data).  We didn't have you train on all the data, because that would take too long.  However, you are free to train your neural network on all the data.  After you complete the project, of course.\n",
    "# Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_tv_script_generation.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
